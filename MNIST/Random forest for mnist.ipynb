{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from CKN import *\n",
    "from Nystrom import *\n",
    "from image_processing_utils import *\n",
    "import scipy.optimize\n",
    "import seaborn as sb\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "test_batch_size=1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "inds=range(60000)\n",
    "import random\n",
    "random.seed(2017)\n",
    "random.shuffle(inds)\n",
    "test=train_loader.dataset.train_data[inds[:3000],:,:]\n",
    "X_labels=train_loader.dataset.train_labels[torch.LongTensor(inds[:3000])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting. This may take a while\n",
      "Confusion matrix:\n",
      "[[2258    1    4    1    2    2    3    1    4    2]\n",
      " [   1 2566    9    1    1    0    0    7    3    0]\n",
      " [   4    1 2280    5    4    0    1    9    8    2]\n",
      " [   0    0   14 2304    1   13    0    6    8    2]\n",
      " [   2    2    2    0 2183    0    7    5    0   10]\n",
      " [   4    0    0   16    3 2026   12    1    4    3]\n",
      " [   7    5    3    0    5    2 2245    0    4    0]\n",
      " [   1    6   11    2    5    1    0 2373    5   13]\n",
      " [   3    9    4    9    4   10    2    3 2166    5]\n",
      " [   3    2    2    6   19    6    0   12   10 2329]]\n",
      "Accuracy: 0.9840\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.89803922 -1.         -0.81960784 -0.96862745\n  0.6         0.98431373  0.69411765  0.89019608  0.6         0.09803922\n -0.94509804 -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.85098039 -0.55294118  0.62352941  0.3254902\n  0.83529412  0.00392157  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.68627451  0.16862745 -0.16078431 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -0.85098039  0.09803922  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373  0.97647059\n  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.\n -0.16078431  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059\n  0.97647059  0.98431373  0.97647059  0.28627451  0.49019608  0.98431373\n  0.97647059  0.97647059  0.97647059 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -0.8745098   0.69411765  0.98431373  0.98431373  1.\n  0.98431373  0.98431373  0.00392157 -0.10588235 -0.30980392 -1.         -1.\n -1.         -0.40392157  0.74901961  0.98431373 -0.50588235 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.77254902  0.97647059  0.97647059  0.97647059\n  0.98431373  0.78039216  0.82745098 -0.82745098 -1.         -1.         -1.\n -1.         -1.         -1.          0.54509804  0.97647059 -0.11372549\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -0.77254902  0.97647059\n  0.97647059  0.97647059  0.61568627 -0.06666667 -0.70980392 -1.         -1.\n -1.         -1.         -1.         -1.         -1.          0.54509804\n  0.97647059  0.25490196 -0.48235294 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.\n -0.96862745  0.39607843  0.97647059  0.97647059 -0.11372549 -1.         -1.\n -1.         -1.         -1.         -1.         -0.6        -0.10588235\n -0.11372549  0.74117647  0.97647059  0.98431373  0.1372549  -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.55294118  0.98431373  0.98431373  0.99215686\n  0.78823529  0.10588235  0.10588235  0.10588235  0.10588235  0.39607843\n  0.98431373  0.99215686  0.98431373  0.98431373  0.98431373  0.10588235\n -0.23921569 -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -0.85098039\n  0.49019608  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373  0.97647059\n  0.97647059  0.97647059  0.30196078 -0.05098039 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.85098039  0.81176471  0.98431373  0.97647059\n  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.89019608 -0.34117647\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -0.01176471\n  0.95294118  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373\n  0.97647059  0.97647059  0.97647059  0.89019608  0.09019608  0.38823529\n  0.97647059  0.98431373  0.73333333 -0.80392157 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -0.74901961  0.79607843  0.98431373  0.99215686  0.98431373  0.91372549\n  0.37254902  0.38039216 -0.01960784 -0.30196078  0.27843137  0.18431373\n -1.         -1.         -1.          0.99215686  0.98431373 -0.56078431\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -0.55294118  0.97647059  0.97647059\n  0.98431373  0.09019608 -0.56078431 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.          0.78823529\n  0.97647059  0.01960784 -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -0.55294118\n  0.97647059  0.97647059  0.81176471 -0.85098039 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n  0.10588235  0.97647059  0.83529412 -0.82745098 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.\n -0.55294118  0.97647059  0.97647059  0.96078431 -0.41176471 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -0.6         0.89019608  0.97647059  0.38823529 -0.97647059 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -0.01176471  0.96078431  0.99215686  0.59215686 -0.08235294\n -0.8745098  -1.         -1.         -1.         -1.         -1.\n -0.64705882  0.10588235  0.89019608  0.99215686  0.9372549   0.86666667\n -0.80392157 -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.          0.76470588\n  0.98431373  0.97647059  0.97647059  0.00392157 -0.05882353  0.12941176\n -0.55294118  0.23137255  0.3254902   0.76470588  0.97647059  0.97647059\n  0.98431373  0.34117647 -0.70196078 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -0.21568627  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373  0.97647059\n  0.97647059  0.97647059  0.81176471 -0.85098039 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -0.28627451  0.68627451\n  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n  0.59215686  0.18431373 -0.09019608 -0.68627451 -0.80392157 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5bca9ad5bbd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5bca9ad5bbd9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5bca9ad5bbd9>\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(clf, data)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Print example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtry_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtry_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clf.predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtry_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.89803922 -1.         -0.81960784 -0.96862745\n  0.6         0.98431373  0.69411765  0.89019608  0.6         0.09803922\n -0.94509804 -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.85098039 -0.55294118  0.62352941  0.3254902\n  0.83529412  0.00392157  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.68627451  0.16862745 -0.16078431 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -0.85098039  0.09803922  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373  0.97647059\n  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.\n -0.16078431  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059\n  0.97647059  0.98431373  0.97647059  0.28627451  0.49019608  0.98431373\n  0.97647059  0.97647059  0.97647059 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -0.8745098   0.69411765  0.98431373  0.98431373  1.\n  0.98431373  0.98431373  0.00392157 -0.10588235 -0.30980392 -1.         -1.\n -1.         -0.40392157  0.74901961  0.98431373 -0.50588235 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.77254902  0.97647059  0.97647059  0.97647059\n  0.98431373  0.78039216  0.82745098 -0.82745098 -1.         -1.         -1.\n -1.         -1.         -1.          0.54509804  0.97647059 -0.11372549\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -0.77254902  0.97647059\n  0.97647059  0.97647059  0.61568627 -0.06666667 -0.70980392 -1.         -1.\n -1.         -1.         -1.         -1.         -1.          0.54509804\n  0.97647059  0.25490196 -0.48235294 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.\n -0.96862745  0.39607843  0.97647059  0.97647059 -0.11372549 -1.         -1.\n -1.         -1.         -1.         -1.         -0.6        -0.10588235\n -0.11372549  0.74117647  0.97647059  0.98431373  0.1372549  -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.55294118  0.98431373  0.98431373  0.99215686\n  0.78823529  0.10588235  0.10588235  0.10588235  0.10588235  0.39607843\n  0.98431373  0.99215686  0.98431373  0.98431373  0.98431373  0.10588235\n -0.23921569 -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -0.85098039\n  0.49019608  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373  0.97647059\n  0.97647059  0.97647059  0.30196078 -0.05098039 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -0.85098039  0.81176471  0.98431373  0.97647059\n  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.89019608 -0.34117647\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -0.01176471\n  0.95294118  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373\n  0.97647059  0.97647059  0.97647059  0.89019608  0.09019608  0.38823529\n  0.97647059  0.98431373  0.73333333 -0.80392157 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -0.74901961  0.79607843  0.98431373  0.99215686  0.98431373  0.91372549\n  0.37254902  0.38039216 -0.01960784 -0.30196078  0.27843137  0.18431373\n -1.         -1.         -1.          0.99215686  0.98431373 -0.56078431\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -0.55294118  0.97647059  0.97647059\n  0.98431373  0.09019608 -0.56078431 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.          0.78823529\n  0.97647059  0.01960784 -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -0.55294118\n  0.97647059  0.97647059  0.81176471 -0.85098039 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n  0.10588235  0.97647059  0.83529412 -0.82745098 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.\n -0.55294118  0.97647059  0.97647059  0.96078431 -0.41176471 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -0.6         0.89019608  0.97647059  0.38823529 -0.97647059 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -0.01176471  0.96078431  0.99215686  0.59215686 -0.08235294\n -0.8745098  -1.         -1.         -1.         -1.         -1.\n -0.64705882  0.10588235  0.89019608  0.99215686  0.9372549   0.86666667\n -0.80392157 -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.          0.76470588\n  0.98431373  0.97647059  0.97647059  0.00392157 -0.05882353  0.12941176\n -0.55294118  0.23137255  0.3254902   0.76470588  0.97647059  0.97647059\n  0.98431373  0.34117647 -0.70196078 -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -0.21568627  0.98431373  0.97647059  0.97647059  0.97647059\n  0.98431373  0.97647059  0.97647059  0.97647059  0.98431373  0.97647059\n  0.97647059  0.97647059  0.81176471 -0.85098039 -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -0.28627451  0.68627451\n  0.97647059  0.97647059  0.98431373  0.97647059  0.97647059  0.97647059\n  0.59215686  0.18431373 -0.09019608 -0.68627451 -0.80392157 -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.         -1.         -1.         -1.\n -1.         -1.         -1.         -1.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a SVM to categorize 28x28 pixel images into digits (MNIST dataset).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrate the retrival of data, training and testing.\"\"\"\n",
    "    data = get_data()\n",
    "\n",
    "    # Get classifier\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC(probability=False,  # cache_size=200,\n",
    "              kernel=\"rbf\", C=2.8, gamma=.0073)\n",
    "\n",
    "    print(\"Start fitting. This may take a while\")\n",
    "\n",
    "    # take all of it - make that number lower for experiments\n",
    "    examples = len(data['train']['X'])\n",
    "    clf.fit(data['train']['X'][:examples], data['train']['y'][:examples])\n",
    "\n",
    "    analyze(clf, data)\n",
    "\n",
    "\n",
    "def analyze(clf, data):\n",
    "    \"\"\"\n",
    "    Analyze how well a classifier performs on data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : classifier object\n",
    "    data : dict\n",
    "    \"\"\"\n",
    "    # Get confusion matrix\n",
    "    from sklearn import metrics\n",
    "    predicted = clf.predict(data['test']['X'])\n",
    "    print(\"Confusion matrix:\\n%s\" %\n",
    "          metrics.confusion_matrix(data['test']['y'],\n",
    "                                   predicted))\n",
    "    print(\"Accuracy: %0.4f\" % metrics.accuracy_score(data['test']['y'],\n",
    "                                                     predicted))\n",
    "\n",
    "    # Print example\n",
    "    try_id = 1\n",
    "    out = clf.predict(data['test']['X'][try_id])  # clf.predict_proba\n",
    "    print(\"out: %s\" % out)\n",
    "    size = int(len(data['test']['X'][try_id])**(0.5))\n",
    "    view_image(data['test']['X'][try_id].reshape((size, size)),\n",
    "               data['test']['y'][try_id])\n",
    "\n",
    "\n",
    "def view_image(image, label=\"\"):\n",
    "    \"\"\"\n",
    "    View a single image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : numpy array\n",
    "        Make sure this is of the shape you want.\n",
    "    label : str\n",
    "    \"\"\"\n",
    "    from matplotlib.pyplot import show, imshow, cm\n",
    "    print(\"Label: %s\" % label)\n",
    "    imshow(image, cmap=cm.gray)\n",
    "    show()\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Get data ready to learn with.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    simple = False\n",
    "    if simple:  # Load the simple, but similar digits dataset\n",
    "        from sklearn.datasets import load_digits\n",
    "        from sklearn.utils import shuffle\n",
    "        digits = load_digits()\n",
    "        x = [np.array(el).flatten() for el in digits.images]\n",
    "        y = digits.target\n",
    "\n",
    "        # Scale data to [-1, 1] - This is of mayor importance!!!\n",
    "        # In this case, I know the range and thus I can (and should) scale\n",
    "        # manually. However, this might not always be the case.\n",
    "        # Then try sklearn.preprocessing.MinMaxScaler or\n",
    "        # sklearn.preprocessing.StandardScaler\n",
    "        x = x/255.0*2 - 1\n",
    "\n",
    "        x, y = shuffle(x, y, random_state=0)\n",
    "\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        data = {'train': {'X': x_train,\n",
    "                          'y': y_train},\n",
    "                'test': {'X': x_test,\n",
    "                         'y': y_test}}\n",
    "    else:  # Load the original dataset\n",
    "        from sklearn.datasets import fetch_mldata\n",
    "        from sklearn.utils import shuffle\n",
    "        mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "        x = mnist.data\n",
    "        y = mnist.target\n",
    "\n",
    "        # Scale data to [-1, 1] - This is of mayor importance!!!\n",
    "        x = x/255.0*2 - 1\n",
    "\n",
    "        x, y = shuffle(x, y, random_state=0)\n",
    "\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=42)\n",
    "        data = {'train': {'X': x_train,\n",
    "                          'y': y_train},\n",
    "                'test': {'X': x_test,\n",
    "                         'y': y_test}}\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
