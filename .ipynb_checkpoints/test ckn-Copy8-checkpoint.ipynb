{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-37c8736d76df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCKN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNystrom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/Dropbox/Pattern-analysis/pattern_detection/CKN.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_patches_2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNystrom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/Dropbox/Pattern-analysis/pattern_detection/Nystrom.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpattern_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/Dropbox/Pattern-analysis/pattern_detection/pattern_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/networkx/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelabel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/networkx/generators/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matlas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree_seq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/networkx/generators/classic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipartite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomplete_bipartite_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetworkXError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/networkx/algorithms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# the `networkx` namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massortativity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipartite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentrality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchordal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/networkx/algorithms/bipartite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mGenerators\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipartite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \"\"\"\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipartite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from CKN import *\n",
    "from Nystrom import *\n",
    "from image_processing_utils import *\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "test_batch_size=1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=train_loader.dataset.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.heatmap(test[0,:,:].numpy())\n",
    "plt.figure()\n",
    "size_patch=5\n",
    "sb.heatmap(rel_distance_patch(size_patch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inpt=test[0,:,:]\n",
    "sx,sy=inpt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size_patch=5\n",
    "data=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size_patch=5\n",
    "patches=extract_patches(data,size_patch, zero_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_patch=[]        \n",
    "distances=[]\n",
    "X=data\n",
    "n_patches_per_graph=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Nystrom():\n",
    "    \n",
    "    def __init__(self,  n_components=50, iter_max=1000,random_state=None,size_patch=5,lr=0.01,batch_size=100):\n",
    "        self.eta = Variable(torch.Tensor(1.0/n_components *(np.ones((n_components,)))),requires_grad=True)\n",
    "        self.W = torch.Tensor(Variable(None))\n",
    "        self.sigma=None\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.iter_max=iter_max\n",
    "        self.print_lag=15\n",
    "        self.lr=lr\n",
    "        self.all_patches=None\n",
    "        self.training_data=None\n",
    "        self.size_patch=size_patch\n",
    "        self.patches=None\n",
    "        self.distances=None\n",
    "        self.batch_size=batch_size\n",
    "    \t\t\n",
    "    def select_training_patches(self,X,n_patches_per_graph=10,patches=None, verbose=True):\n",
    "        #### X should be  a tensor (3D)\n",
    "        #### select training patches within the same graph\n",
    "       # X_train=torch.Tensor((X.size()[0],n_patches_per_graph,size_patch**2,size_patch**2))\n",
    "    ### either randomly extract patches or samples from a patch list\n",
    "        id_patch=[]        \n",
    "        distances=[]\n",
    "        \n",
    "        for i in range(X.size()[0]):            ## select at random 2 nodes in the adjacency matrix:\n",
    "            a=X.size()[1]\n",
    "            for j in range(n_patches_per_graph):\n",
    "                nx,ny=np.random.choice(range(X.size()[1]*X.size()[2]),2)\n",
    "                distances+=[(nx%a-ny%a)**\n",
    "                            \n",
    "                            2+(nx//a-ny//a)**2]\n",
    "                id_patch+=[[i,nx,ny]]\n",
    "        if patches==None:\n",
    "            patches=extract_patches(X,size_patch, zero_padding=True)\n",
    "            self.all_patches=patches\n",
    "            if verbose: print(\"patches extracted\")\n",
    "        selected_patches=torch.Tensor(len(id_patch),2,size_patch**2)\n",
    "        for j in range(len(id_patch)):\n",
    "\n",
    "            while torch.sum(patches[id_patch[j][1],id_patch[j][0],:])==0 and torch.sum(patches[id_patch[j][2],id_patch[j][0],:])==0:\n",
    "                nx,ny=np.random.choice(range(X.size()[1]*X.size()[2]),2)\n",
    "                id_patch[j]=[id_patch[j][0],nx,ny]\n",
    "            selected_patches[j,0,:]=patches[id_patch[j][1],id_patch[j][0],:]\n",
    "            selected_patches[j,1,:]=patches[id_patch[j][2],id_patch[j][0],:]\n",
    "            \n",
    "        self.training_data=selected_patches\n",
    "            \n",
    "        return selected_patches\n",
    "            \n",
    "        \n",
    "    def init_W(self,X):\n",
    "        self.training_data=self.select_training_patches(X,50)\n",
    "        distances2=torch.sum((self.training_data[:,0,:]-self.training_data[:,1,:])**2,dim=1)\n",
    "        if self.sigma==None:\n",
    "        \t### set to be quantile\n",
    "            ### compute the distance between patches\n",
    "            self.sigma=np.percentile(distances2.numpy(),10)\n",
    "        X_tilde=torch.cat((self.training_data[:,0,:],self.training_data[:,1,:]), dim=0)\n",
    "        print(self.sigma)\n",
    "        km=KMeans(n_clusters=self.n_components)\n",
    "        inds=range(int(X_tilde.size()[0]))\n",
    "        np.random.shuffle(inds)\n",
    "        X_tilde=X_tilde[list(inds[:np.min([4000,len(inds)])]),:]\n",
    "        km.fit(X_tilde.numpy())\n",
    "        self.W=Variable(torch.Tensor(km.cluster_centers_), requires_grad=True)\n",
    "    \t\t\t\n",
    "    def fit(self, X, y=None, init=True):\n",
    "        rnd = check_random_state(self.random_state)\n",
    "        D=self.n_components\n",
    "        if init==True: self.init_W(X)\n",
    "        X_input=Variable(self.training_data, requires_grad=False)\n",
    "        N=X_input.size()[0]\n",
    "        n=X_input.size()[1]\n",
    "        expected_output=Variable(torch.Tensor(torch.exp(-torch.sum((X_input[:,0,:]-X_input[:,1,:])**2/self.sigma,1))),requires_grad=False)\n",
    "        loss_func = nn.MSELoss()\n",
    "        optimizer = optim.SGD([Cell.W,Cell.eta],lr=0.01) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "        batch_nb=N//self.batch_size\n",
    "        batch_size=self.batch_size\n",
    "        p_size=size_patch\n",
    "        output_tot=torch.Tensor()\n",
    "\n",
    "        for t in range(Cell.iter_max):\n",
    "                overall_loss=0\n",
    "            #def closure():\n",
    "                for b in range(batch_nb):\n",
    "                    #Cell.eta=F.relu(Cell.eta)\n",
    "                    XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    output=(XX-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "                    #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "                    #output2=output.view(N,D,n)\n",
    "                    output=torch.sum(output,2)\n",
    "                    #print(output[:10])\n",
    "                    output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/Cell.sigma]))).expand_as(output)*output)\n",
    "\n",
    "                    #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "                    #weights=C.expand_as(eta)*F.softmax(-eta)\n",
    "                    #STOP\n",
    "                    output2=torch.matmul(F.relu(Cell.eta),output2)\n",
    "                    #print(output2[:10])\n",
    "                    loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])+torch.sum((F.relu(Cell.eta)-Cell.eta)**2)\n",
    "                    #+0.1/D*torch.sum(Cell.eta)\n",
    "                    #+2.0/D*torch.dot(Variable(torch.Tensor([1]*D)),Cell.eta)\n",
    "                    overall_loss+=loss[0]\n",
    "                    #STOP\n",
    "                    #if t%Cell.print_lag==0: print(t, loss.data[0])\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "                    #Cell.eta=F.relu(Cell.eta)\n",
    "                    #print(b)\n",
    "\n",
    "                #optimizer.step()\n",
    "                #Cell.eta=(Cell.eta).clamp(0,1000)\n",
    "\n",
    "                print(t,overall_loss/(batch_nb))\n",
    "                #print(eta[:5])\n",
    "\n",
    "    \n",
    "\n",
    "    def fit_LBFGS(self,X,init=True):\n",
    "        rnd = check_random_state(self.random_state)\n",
    "        D=self.n_components\n",
    "        if init==True: self.init_W(X)\n",
    "        print(\"Initialization: done\")\n",
    "        D=Cell.n_components\n",
    "        X_input=self.training_data\n",
    "        N=X_input.size()[0]\n",
    "        n=X_input.size()[1]\n",
    "        m=size_patch**2\n",
    "        expected_output=Variable(torch.exp(torch.Tensor([-1.0/(2*self.sigma)])*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1)),requires_grad=False)\n",
    "        X_input=Variable(X_input, requires_grad=False)\n",
    "        loss_func = nn.MSELoss()\n",
    "        optimizer = optim.LBFGS([self.W,self.eta]) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "        batch_size=self.batch_size\n",
    "        sigma=self.sigma\n",
    "        output_tot=torch.Tensor()\n",
    "        for t in range(self.iter_max):\n",
    "            overall_loss=0\n",
    "            for b in range(batch_size):\n",
    "                def closure():\n",
    "                    XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    output=(XX-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "                    \n",
    "                    output=torch.sum(output.view(-1,p_size**2),1)\n",
    "                    output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output)\n",
    "                    #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "                    weights=(2.0/(math.pi*sigma**2))**(D/2)*F.softmax(-self.eta)\n",
    "                    output2=torch.matmul(output2.view(-1,D),weights)\n",
    "                    loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])\n",
    "                    #overall_loss+=loss[0]\n",
    "                    #if t%self.print_lag==0: print(t, loss.data[0])\n",
    "                    optimizer.zero_grad() \n",
    "                    loss.backward()\n",
    "                    #optimizer.step()\n",
    "                    \n",
    "                optimizer.step(closure)\n",
    "                self.eta=(self.eta).clamp(0,1000)\n",
    "            print(t,overall_loss)\n",
    "        \n",
    "    def _get_kernel_params(self):\n",
    "        params = self.kernel_params\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=data.type(torch.FloatTensor)\n",
    "data2=data-torch.mean(data.view(-1,data.size()[1]*data.size()[2]),0).view(28,28)\n",
    "S=torch.std(data2.view(-1,data.size()[1]*data.size()[2]),0)\n",
    "S[S==0]=1\n",
    "data2=1.0/S*data2.view(-1,data.size()[1]*data.size()[2])\n",
    "data2=data2.view(-1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cell=Nystrom()\n",
    "Cell.init_W(data2)\n",
    "#Cell.fit_LBFGS(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params=[Cell.training_data,Cell.sigma]\n",
    "W_stor=Cell.W.clone()\n",
    "eta_stor=Cell.eta.clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distances=torch.sum((Cell.training_data[:,0,:]-Cell.training_data\n",
    "                     [:,1,:])**2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.heatmap(Cell.training_data[0,1,:].view(5,5).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D=Cell.n_components\n",
    "X_input=Cell.training_data\n",
    "N=X_input.size()[0]\n",
    "n=X_input.size()[1]\n",
    "m=size_patch**2\n",
    "expected_output=Variable(torch.exp(torch.Tensor([-1.0/(2*Cell.sigma)])*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1)),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(expected_output.data.numpy(),color='red')\n",
    "plt.hist(output2.data.numpy(),color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#expected_output=expected_output/torch.sqrt(torch.median(expected_output**2))\n",
    "#X_input=Variable(X_input, requires_grad=False)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.SGD([Cell.W,Cell.eta],lr=0.01) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "batch_nb=500\n",
    "batch_size=N//batch_nb\n",
    "p_size=size_patch\n",
    "output_tot=torch.Tensor()\n",
    "\n",
    "for t in range(Cell.iter_max):\n",
    "        overall_loss=0\n",
    "    #def closure():\n",
    "        for b in range(batch_nb):\n",
    "            #Cell.eta=F.relu(Cell.eta)\n",
    "            XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "            YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "            output=(XX-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "            #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "            #output2=output.view(N,D,n)\n",
    "            output=torch.sum(output,2)\n",
    "            #print(output[:10])\n",
    "            output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/Cell.sigma]))).expand_as(output)*output)\n",
    "            \n",
    "            #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "            #weights=C.expand_as(eta)*F.softmax(-eta)\n",
    "            #STOP\n",
    "            output2=torch.matmul(F.relu(Cell.eta),output2)\n",
    "            #print(output2[:10])\n",
    "            loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])+torch.sum((F.relu(Cell.eta)-Cell.eta)**2)\n",
    "            #+0.1/D*torch.sum(Cell.eta)\n",
    "            #+2.0/D*torch.dot(Variable(torch.Tensor([1]*D)),Cell.eta)\n",
    "            overall_loss+=loss[0]\n",
    "            #STOP\n",
    "            #if t%Cell.print_lag==0: print(t, loss.data[0])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            #Cell.eta=F.relu(Cell.eta)\n",
    "            #print(b)\n",
    "            \n",
    "        #optimizer.step()\n",
    "        #Cell.eta=(Cell.eta).clamp(0,1000)\n",
    "        \n",
    "        print(t,overall_loss/(batch_nb))\n",
    "        #print(eta[:5])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output=(XX-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "            #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "            #output2=output.view(N,D,n)\n",
    "#output=torch.sum(output.view(-1,p_size**2),1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output=torch.sum(output,2)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/Cell.sigma]))).expand_as(output)*output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cell.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XX=X_input[:1000,0,:].data.expand(D,1000,25)\n",
    "YY=X_input[:1000,1,:].data.expand(D,1000,25)\n",
    "WW=Cell.W.data.contiguous().view(D,1,25).expand(50,1000,25)\n",
    "Yhat=torch.exp(torch.Tensor([-1.0/(Cell.sigma)]).expand(D,1000)*\\\n",
    "               (torch.sum((XX-WW)**2,2)+torch.sum((YY-WW)**2,2)))\n",
    "Yhat=torch.matmul(Cell.eta.data,Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WW=Cell.W.data.contiguous().view(D,1,25).expand(50,1000,25)\n",
    "test=torch.exp(torch.Tensor([-1.0/(Cell.sigma)]).expand(D,1000)*\\\n",
    "               (torch.sum((XX-WW)**2,2)+torch.sum((YY-WW)**2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(Y.numpy(),Yhat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.sum((X_input[:1000,0,:]\\\n",
    "                                          -X_input[:1000,1,:])**2,1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output3=torch.matmul(F.relu(Cell.eta),output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(expected_output[b*batch_size:(b+1)*batch_size].data.numpy(),output3.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(expected_output[b*batch_size:(b+1)*batch_size].data.numpy(), color='red')\n",
    "plt.hist(output2.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected_output/torch.sqrt(torch.median(expected_output**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cell.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func(params, *args):\n",
    "    D=args[4]\n",
    "    #print(params)\n",
    "    W = params[:-D]\n",
    "    eta=params[-D:]\n",
    "    \n",
    "    W=W.reshape([D,-1])\n",
    "    eta=eta.reshape([D,1])\n",
    "    #print(W.shape)\n",
    "    XX = args[0]\n",
    "    YY = args[1]  #.reshape([100,25,1]),D)\n",
    "    sigma=args[2]\n",
    "    N=XX.shape[0]\n",
    "    expected_ouput = args[3]\n",
    "    test=np.array([((XX[i,:]-W)**2)+((YY[i,:]-W)**2)  for i in range(N)])\n",
    "    #print((np.exp(-1.0/sigma*np.sum(test,2))).shape)\n",
    "    #print(eta.shape)\n",
    "    res=(expected_ouput_t -((eta.T).dot(np.exp(-1.0/sigma*np.sum(test,2)))))\n",
    "    return np.sum(res**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expected_output=Variable(torch.exp(Variable(torch.Tensor([-1.0/(2*sigma)]))*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1)),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testy2=np.array([((testy[i,:]-W.data.numpy())**2)+((testy[i,:]-W.data.numpy())**2) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cell.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args=(X_input[b*batch_size:(b+1)*batch_size,0,:].data.numpy(),\\\n",
    "      X_input[b*batch_size:(b+1)*batch_size,1,:].data.numpy(),\\\n",
    "      Cell.sigma,\\\n",
    "      expected_ouput_t,\\\n",
    "      100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_values = np.array(list(Cell.W.data.numpy().reshape([-1,]))+list(Cell.eta.data))\n",
    "mybounds = [(None,None)]*2500+[(0,None)]*100\n",
    "x,f,d = scipy.optimize.fmin_l_bfgs_b(func, x0=initial_values, args=args, bounds=mybounds,approx_grad=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mybounds = [(None,None)*np.ones(Cell.W.data.shape), [(0,None)]*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.exp(torch.Tensor([-1.0/(2*sigma)])*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N=XX.shape[0]\n",
    "test=np.array([((W.numpy()-XX[i,:])**2)+((YY[i,:]-W.numpy())**2)  for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res=(expected_ouput_t -((eta.numpy().T).dot(np.exp(-1.0/sigma*np.sum(test,2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Wfinal=x[:-D].reshape([D,25])\n",
    "etafinal=x[-D:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=np.array([((Wfinal-XX[i,:])**2)+((YY[i,:]-Wfinal)**2)  for i in range(N)])\n",
    "testy2=((etafinal.T).dot(np.exp(-1.0/sigma*np.sum(test,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### convergence feels super long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigma=Cell.sigma\n",
    "XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "output=(XX-W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "            #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "            #output2=output.view(N,D,n)\n",
    "output=torch.sum(output.view(-1,p_size**2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XX,YY,sigma=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([Cell.W,Cell.eta], lr=0.001) # instantiate optimizer with model params + learning rate (SGD for the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "#for b in range(batch_size):\n",
    "if True:\n",
    "    XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "    YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "    output=(XX-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "    #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "    #output2=output.view(N,D,n)\n",
    "    output=torch.sum(output.view(-1,p_size**2),1)\n",
    "    output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output)\n",
    "    #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "    output2=torch.matmul(output2.view(-1,D),Cell.eta)\n",
    "    loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])\n",
    "    if t%Cell.print_lag==0: print(t, loss.data[0])\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    #optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output=torch.sum(output.view(-1,p_size**2),1)\n",
    "output2=Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cell.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size_image=data.size()[1:]\n",
    "selected_patches=torch.Tensor(len(id_patch),2,size_patch**2)\n",
    "for j in range(len(id_patch)):\n",
    "        selected_patches[j,0,:]=patches[id_patch[j][1],id_patch[j][0],:]\n",
    "        selected_patches[j,1,:]=patches[id_patch[j][2],id_patch[j][0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_selected_patches(data,id_patch,size_patch, zero_padding=True):\n",
    "    \n",
    "    size_image=data.size()[1:]\n",
    "    center_x=id_patch[1]//size_image[0]\n",
    "    center_y=id_patch[1]%size_image[0]\n",
    "    selected_patches=torch.Tensor((len(id_patches),2,size_patch**2))\n",
    "    patches=extract_patches(data,size_patch, zero_padding=True)\n",
    "    for j in range(len(id_patch)):\n",
    "        selected_patches[j,0,:]=patches[center_x[j],id_patch[j][0],:]\n",
    "        selected_patches[j,1,:]=patches[center_y[j],id_patch[j][0],:]\n",
    "    return patches[id_patch[0],center_x,center_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "size_image=data.size()[1:]\n",
    "a=size_patch+size_image[0]\n",
    "b=size_patch+size_image[1]\n",
    "padded_image=torch.Tensor(np.zeros((data.size()[0],a,b)))\n",
    "padded_image[:,size_patch//2:(size_patch//2)+size_image[0],size_patch//2:(size_patch//2)+size_image[1]]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx,ny=padded_image.size()[1:]\n",
    "patches=torch.Tensor(np.array([padded_image[:,ii:ii+size_patch,jj:jj+size_patch].numpy() for ii in np.arange(0,nx-size_patch,1) for jj in np.arange(0,ny-size_patch,1) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patches=patches.view([patches.size()[0],patches.size()[1],size_patch**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=test\n",
    "size_patch=5\n",
    "import numpy as np\n",
    "patches=extract_patches(data,size_patch, zero_padding=True)\n",
    "patches=patches.view([patches.size()[0],patches.size()[1],size_patch**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_shape=size_patch\n",
    "#spacing=np.min([p_shape, data.size()[2]//p_shape]) \n",
    "spacing=p_shape//2\n",
    "gamma=2\n",
    "D=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patches[100,30,:].norm(p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W=torch.Tensor(np.random.sample(D*size_patch**2).reshape([D,size_patch**2]))\n",
    "\n",
    "eta=torch.Tensor(np.random.dirichlet([1.0/D]*D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_p,n_d=patches.size()[:2]\n",
    "norm = torch.Tensor(np.array([patches[i,j,:].norm(p=2) for i in range(n_p) for j in range(n_d)]))\n",
    "\n",
    "norm2 = patches.view((n_p*n_d,size_patch**2)).norm(p=2,dim=1).view((n_p,n_d,1))\n",
    "\n",
    "norm3=torch.max(norm2, torch.Tensor(np.ones(norm2.size())))\n",
    "\n",
    "norm_patches=patches.div(norm3.expand_as(patches))\n",
    "\n",
    "W2=W.expand(n_d*n_p,D,25)\n",
    "norm_patches2=norm_patches.view((n_d*n_p,1,size_patch**2)).expand(n_d*n_p,D,size_patch**2)\n",
    "batch_size=10000\n",
    "testy=torch.Tensor((n_d*n_p*D))\n",
    "output2=torch.Tensor((n_d*n_p*D))\n",
    "sigma=1\n",
    "\n",
    "\n",
    "\n",
    "for b in range(n_d*n_p//batch_size):\n",
    "    print(b)\n",
    "    testy[b*batch_size*D:(b+1)*batch_size*D]=torch.sum(((norm_patches2[b*batch_size:(b+1)*batch_size,:,:]\\\n",
    "            -W.expand_as(norm_patches2[b*batch_size:(b+1)*batch_size,:,:]))**2).contiguous().view((-1,25)),dim=1)\n",
    "    \n",
    "    output2[b*batch_size*D:(b+1)*batch_size*D]=torch.exp(-1.0/sigma*testy[b*batch_size*D:(b+1)*batch_size*D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output2[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output2=testy.view(n_p,n_d,D)\n",
    "sigma=1\n",
    "output2=torch.exp(-1.0/sigma*output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output2=torch.matmul(output2,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zeta=norm.view((n_d,n_p))*(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#output2=torch.matmul(output2,eta)\n",
    "#zeta=norm.mmul(output2)\n",
    "#patches=[c.reshape([1,-1]) for c in extract_patches_2d(zeta.numpy(),[p_shape,p_shape])]\n",
    "beta=gamma*spacing  ### what is that spacing? Where is it defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_map=2.0/np.sqrt(math.pi)*torch.sum(torch.exp(-1.0/beta*torch.Tensor(model_patch))*zeta,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output2=output.view(N,p_shape**2,D)\n",
    "output2=torch.exp(-1.0/self.Kernels[id_map].sigma*torch.sum(output2,1))\n",
    "output2=torch.matmul(output2,self.Kernels[id_map].eta)\n",
    "zeta=norm.mmul(output2)\n",
    "patches=[c.reshape([1,-1]) for c in extract_patches_2d(zeta.numpy(),[p_shape,p_shape])]\n",
    "beta=gamma*spacing  ### what is that spacing? Where is it defined?\n",
    "output_map=self.model_patch[k]*patches\n",
    "return output_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    ### start by extracting patches from image\n",
    "    patches=[c.reshape([1,-1]) for c in extract_patches_2d(xi.numpy(),[p_shape,p_shape])]\n",
    "    patches=torch.Tensor(patches)\n",
    "    patches=patches.squeeze()\n",
    "    ### subsamples patches?\n",
    "    \n",
    "    ### l2 normalized version of the patches\n",
    "    norm = patches.norm(p=2, dim=0, keepdim=True)\n",
    "    norm_patches=patches.div(norm.expand_as(patches))\n",
    "    N=norm_patches.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_activation_map(im,xi,id_map):\n",
    "    ### have to generalize this guy to take as input lots of images in the right tensor shape\n",
    "    p_shape=self.patch_shape[id_map]\n",
    "    spacing=np.min([im.size()[0]//p_shape, im.size()[1]//p_shape]) \n",
    "    gamma=2\n",
    "    D=self.Kernels[id_map].n_components\n",
    "    ### start by extracting patches from image\n",
    "    patches=[c.reshape([1,-1]) for c in extract_patches_2d(xi.numpy(),[p_shape,p_shape])]\n",
    "    patches=torch.Tensor(patches)\n",
    "    patches=patches.squeeze()\n",
    "    ### subsamples patches?\n",
    "    \n",
    "    ### l2 normalized version of the patches\n",
    "    norm = patches.norm(p=2, dim=0, keepdim=True)\n",
    "    norm_patches=patches.div(norm.expand_as(patches))\n",
    "    N=norm_patches.size()[0]\n",
    "    output=(norm_patches.repeat(1, D).view(N,p_shape**2,D)-self.Kernels[id_map].W.view(p_shape**2,D).repeat(0, N))**2\n",
    "    output2=output.view(N,p_shape**2,D)\n",
    "    output2=torch.exp(-1.0/self.Kernels[id_map].sigma*torch.sum(output2,1))\n",
    "    output2=torch.matmul(output2,self.Kernels[id_map].eta)\n",
    "    zeta=norm.mmul(output2)\n",
    "    patches=[c.reshape([1,-1]) for c in extract_patches_2d(zeta.numpy(),[p_shape,p_shape])]\n",
    "    beta=gamma*spacing  ### what is that spacing? Where is it defined?\n",
    "    output_map=self.model_patch[k]*patches\n",
    "    return output_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from Nystrom import *\n",
    "\n",
    "# CNN Model\n",
    "class CKN(nn.Module):\n",
    "    def __init__(self, n_components=100,n_layers=3,n_patches=[5,2,0],subsampling_factors=[2,2,0],type_zero_layer=2,quantiles=[0.1,0.1,0.1],batch_size=[10000,200,50]):\n",
    "        super(CKN, self).__init__()\n",
    "        self.Kernel={k:Nystrom(n_components=n_components) for k in range(n_layers)}\n",
    "        self.n_patches=n_patches\n",
    "        self.n_layers=3\n",
    "        self.subsampling_factors=subsampling_factors\n",
    "        self.size_maps=size_maps\n",
    "        self.type_zero_layer=type_zero_layer                \n",
    "          # 0: extract patches from the image, substracting the mean color (or mean intensity)\n",
    "          #    and performing contrast normalization\n",
    "          # 1: extract patches from the image, substracting the mean color (or mean intensity)\n",
    "          # 2: extract patches from the image, without substracting the mean color (or mean intensity)\n",
    "          # 3: subsample orientations from the two-dimensional gradient at every pixel, \n",
    "          #    it requires npatches of the form [1 * *] and a recommended value for size_maps is\n",
    "          #    size_maps=[12 * *]  (subsample 12 orientations)\n",
    "        self.quantiles=quantiles\n",
    "        self.model_patch= {k:rel_distance_patch(n_patches[k])for k in range(n_layers)}\n",
    "        self.batch_size=batch_size\n",
    "\t\n",
    "    def init_params(X):\n",
    "        self.Kernel[0].init(X)\n",
    "\t\n",
    "\n",
    "    def train0(self, X):\n",
    "        self.Kernel[0].fit(X)\n",
    "    \n",
    "    \n",
    "    def train_layer_k(self, id_layer, input):\n",
    "        self.Kernel[id_layer].fit(input)\n",
    "        output=self.get_activation_map(im,xi,id_map)\n",
    "    \t\n",
    "    def train_network(X):\n",
    "        input_map=X\n",
    "        for k in self.n_layers:\n",
    "            self.Kernel[k].init(input_map)\n",
    "            self.Kernel[k].fit(input_map)\n",
    "            input_map=self.get_activation_map(input_map,k)\n",
    "        return input_map\n",
    "    \n",
    "    \t \t\n",
    "    def get_activation_map(self,input_map,id_map):\n",
    "        ### have to generalize this guy to take as input lots of images in the right tensor shape\n",
    "        dim=np.sqrt(input_map.size()[0])\n",
    "        p_shape=self.patch_shape[id_map]\n",
    "        spacing=np.min([input_map.size()[0]//p_shape,input_map.size()[1]//p_shape]) \n",
    "        gamma=self.subsampling_factors[id_map]\n",
    "        D=self.Kernels[id_map].n_components\n",
    "        sigma=self.Kernels[id_map].sigma\n",
    "        ### start by extracting patches from image\n",
    "        patches=extract_patches(input_map,size_patch)\n",
    "        n_p,n_d=patches.size()[:2]\n",
    "        #norm = torch.Tensor(np.array([patches[i,j,:].norm(p=2) for i in range(n_p) for j in range(n_d)]))\n",
    "        beta=gamma*spacing\n",
    "        mpatches=extract_patch_mask(batch_size,[dim,dim],size_patch, beta=beta,zero_padding=True)\n",
    "        mpatches=mpatches.view(-1,dim*dim)\n",
    "        selected_pixels=[i*dim+j for j in np.arange(0,dim, 2) for i in np.arange(0,dim, gamma)]\n",
    "        output_map=torch.Tensor(len(selected_pixels),n_d,D)\n",
    "        batch_size=100\n",
    "        for b in range(N//batch_size):\n",
    "            print(b)\n",
    "            temp=torch.Tensor(D,n_p*batch_size)\n",
    "            zeta=torch.Tensor(D,n_p*batch_size)\n",
    "            XX=norm_patches[:,b*batch_size:(b+1)*batch_size,:].contiguous().view(1,n_p*batch_size,p_shape**2).expand(D,n_p*batch_size,p_shape**2)\n",
    "            WW=Cell.W.contiguous().view(D,1,p_shape**2).expand_as(XX)\n",
    "            w=torch.sqrt(Cell.eta.view(D,1).data.expand(D,XX.size()[1])) *\\\n",
    "                    torch.exp(torch.div(torch.sum((XX-WW.data)**2,dim=2),-1.0/sigma))\n",
    "           # temp[:,b*batch_size*n_p:(b+1)*batch_size*n_p]=w\n",
    "            temp=w\n",
    "            Reg=norm2[:,b*batch_size:(b+1)*batch_size].squeeze().contiguous().view(n_p*batch_size,1)\n",
    "\n",
    "            zeta=torch.t(Reg)*temp#output2=output.view(n_d*n_p,D,p_shape**2)\n",
    "            #output2=torch.exp(-1.0/self.Kernels[id_map].sigma*torch.sum(output2,1))\n",
    "            #output2=torch.matmul(output2,self.Kernels[id_map].eta)\n",
    "            zeta=zeta.view(D,n_p,batch_size)\n",
    "            zeta2=zeta.permute(1,2,0)\n",
    "            test=torch.matmul(mpatches,zeta2.contiguous().view(zeta2.size()[0],zeta2.size()[1]*zeta2.size()[2]))\n",
    "            out=test.view(zeta2.size()[0],zeta2.size()[1],zeta2.size()[2])\n",
    "            output_map[:,b*batch_size:(b+1)*batch_size,:]=out[selected_pixels,:,:]        \n",
    " \n",
    "        return output_map\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_shape=Cell.size_patch\n",
    "input_map=data2\n",
    "spacing=np.min([input_map.size()[1]//p_shape,input_map.size()[2]//p_shape]) \n",
    "gamma=2\n",
    "\n",
    "#D=self.Kernels[id_map].n_components\n",
    "### start by extracting patches from image\n",
    "#patches=extract_patches(input_map,size_patch)\n",
    "size_patch=5\n",
    "patches=extract_patches(input_map,size_patch)\n",
    "\n",
    "\n",
    "n_p,n_d=patches.size()[:2]\n",
    "#norm = torch.Tensor(np.array([patches[i,j,:].norm(p=2) for i in range(n_p) for j in range(n_d)]))\n",
    "norm2 = patches.view((n_p*n_d,size_patch**2)).norm(p=2,dim=1).view((n_p,n_d,1))\n",
    "norm3 = norm2.clone()\n",
    "norm3[norm3==0]=1\n",
    "norm_patches=torch.div(patches.view(n_p,n_d,size_patch**2),\\\n",
    "                      norm3.expand(n_p,n_d,size_patch**2)) ###psi_tilde\n",
    "batch_size=100\n",
    "temp=torch.Tensor(D,n_d*n_p)\n",
    "zeta=torch.Tensor(D,n_d*n_p)\n",
    "sigma=Cell.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for b in range(n_d//batch_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XX=norm_patches[:,b*batch_size:(b+1)*batch_size,:].\\\n",
    "contiguous().view(1,n_p*batch_size,p_shape**2).\\\n",
    "expand(D,n_p*batch_size,p_shape**2)\n",
    "WW=Cell.W.contiguous().view(D,1,p_shape**2).expand_as(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v=torch.exp(torch.div(torch.sum((XX-WW.data)**2,dim=2),-1.0/sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t=torch.sqrt(Cell.eta.view(D,1).data.expand(D,XX.size()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v=torch.exp(torch.div(torch.sum((XX-WW.data)**2,dim=2),-1.0/sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "XX=norm_patches[:,b*batch_size:(b+1)*batch_size,:].contiguous().view(1,n_p*batch_size,p_shape**2).expand(D,n_p*batch_size,p_shape**2)\n",
    "WW=Cell.W.contiguous().view(D,1,p_shape**2).expand_as(XX)\n",
    "w=torch.sqrt(Cell.eta.view(D,1).data.expand(D,XX.size()[1])) *\\\n",
    "            torch.exp(torch.div(torch.sum((XX-WW.data)**2,dim=2),-1.0/sigma))\n",
    "temp[:,b*batch_size*n_p:(b+1)*batch_size*n_p]=w\n",
    "\n",
    "Reg=norm2[:,b*batch_size:(b+1)*batch_size].squeeze().contiguous().view(n_p*batch_size,1)\n",
    "\n",
    "zeta[:,b*batch_size*n_p:(b+1)*batch_size*n_p]=torch.t(Reg) *\\\n",
    "temp[:,b*batch_size*n_p:(b+1)*batch_size*n_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(selected_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_patches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpatches=extract_patch_mask(batch_size,[28,28],size_patch, zero_padding=True)\n",
    "mpatches=mpatches.view(-1,28*28)\n",
    "selected_pixels=[i*28+j for j in np.arange(0,28, 2) for i in np.arange(0,28, 2)]\n",
    "output_map=torch.Tensor(len(selected_pixels),n_d,D)\n",
    "batch_size=100\n",
    "for b in range(N//batch_size):\n",
    "    print(b)\n",
    "    temp=torch.Tensor(D,n_p*batch_size)\n",
    "    zeta=torch.Tensor(D,n_p*batch_size)\n",
    "    XX=norm_patches[:,b*batch_size:(b+1)*batch_size,:].contiguous().view(1,n_p*batch_size,p_shape**2).expand(D,n_p*batch_size,p_shape**2)\n",
    "    WW=Cell.W.contiguous().view(D,1,p_shape**2).expand_as(XX)\n",
    "    w=torch.sqrt(Cell.eta.view(D,1).data.expand(D,XX.size()[1])) *\\\n",
    "            torch.exp(torch.div(torch.sum((XX-WW.data)**2,dim=2),-1.0/sigma))\n",
    "   # temp[:,b*batch_size*n_p:(b+1)*batch_size*n_p]=w\n",
    "    temp=w\n",
    "    Reg=norm2[:,b*batch_size:(b+1)*batch_size].squeeze().contiguous().view(n_p*batch_size,1)\n",
    "\n",
    "    zeta=torch.t(Reg)*temp#output2=output.view(n_d*n_p,D,p_shape**2)\n",
    "    #output2=torch.exp(-1.0/self.Kernels[id_map].sigma*torch.sum(output2,1))\n",
    "    #output2=torch.matmul(output2,self.Kernels[id_map].eta)\n",
    "    zeta=zeta.view(D,n_p,batch_size)\n",
    "    zeta2=zeta.permute(1,2,0)\n",
    "    test=torch.matmul(mpatches,zeta2.contiguous().view(zeta2.size()[0],zeta2.size()[1]*zeta2.size()[2]))\n",
    "    out=test.view(zeta2.size()[0],zeta2.size()[1],zeta2.size()[2])\n",
    "    output_map[:,b*batch_size:(b+1)*batch_size,:]=out[selected_pixels,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_patch_mask(N,size_image,size_patch, beta=1,zero_padding=True):\n",
    "    a=2*np.max([size_patch//2,1])+size_image[0]\n",
    "    b=2*np.max([size_patch//2,1])+size_image[1]\n",
    "    padded_image=np.zeros((a,b))\n",
    "    padded_image[np.max([size_patch//2,1]):np.max([size_patch//2,1])+size_image[0],np.max([size_patch//2,1]):(np.max([size_patch//2,1]))+size_image[1]]=1\n",
    "    nx,ny=padded_image.shape\n",
    "    print(nx,ny)\n",
    "    patches=torch.Tensor(np.array([(padded_image*create_distance_mask(size_image,size_patch,ii,jj,beta=beta))[np.max([size_patch//2,1]):np.max([size_patch//2,1])+size_image[0],np.max([size_patch//2,1]):(np.max([size_patch//2,1]))+size_image[1]] for ii in np.arange(0,nx-2*np.max([size_patch//2,1])) for jj in np.arange(0,ny-2*np.max([size_patch//2,1])) ]))\n",
    "    return patches\n",
    "\n",
    "\n",
    "def create_distance_mask(size_image,size_patch,ii,jj,beta=1):\n",
    "    image=np.zeros((size_image[0]+2*np.max([size_patch//2,1]),size_image[1]+2*np.max([size_patch//2,1])))\n",
    "    mask=rel_distance_patch(size_patch).numpy()\n",
    "    image[ii:ii+size_patch,jj:jj+size_patch]=np.exp(-1.0/beta**2*mask)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_image=[28,28]\n",
    "a=2*np.max([size_patch//2,1])+size_image[0]\n",
    "b=2*np.max([size_patch//2,1])+size_image[1]\n",
    "padded_image=np.zeros((a,b))\n",
    "padded_image[np.max([size_patch//2,1]):np.max([size_patch//2,1])+size_image[0],np.max([size_patch//2,1]):(np.max([size_patch//2,1]))+size_image[1]]=1\n",
    "nx,ny=padded_image.shape\n",
    "mask=rel_distance_patch(size_patch).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_map.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.heatmap(mpatches[60,:,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpatches=extract_patch_mask(10*batch_size,[28,28],size_patch, zero_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpatches=mpatches.view(mpatches.size()[0],mpatches.size()[1]*mpatches.size()[2])\n",
    "mpatches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zeta=zeta.view(D, n_p,10*batch_size)\n",
    "zeta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.view(zeta.size()[0],zeta.size()[1],zeta.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_map.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=torch.matmul(mpatches,zeta2.contiguous().view(zeta2.size()[0],zeta2.size()[1]*zeta2.size()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_pixels=[i*28+j for j in np.arange(0,28, 2) for i in np.arange(0,28, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=np.zeros((28,28))\n",
    "test.reshape([-1,])[selected_pixels]=1\n",
    "sb.heatmap(test.reshape([28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_map.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zeta=zeta.view(D, n_p, 10, size_patch,size_patch)\n",
    "zeta=zeta.view((n_d,im.size()[0],im.size()[1]))\n",
    "selected_pixels=[i*28+j for j in np.arange(0,28, 2) for i in np.arange(0,28, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_map2=output_map[selected_pixels,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_map.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
