{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import networkx as nx\n",
    "import os\n",
    "import sys\n",
    "sys.path.append( '/Users/cdonnat/Dropbox/Pattern-analysis/')\n",
    "import matplotlib.pyplot as plt\n",
    "from shapes.shapes import *\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import seaborn as sb\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append( '../../Pattern-analysis/')\n",
    "from shapes.shapes import *\n",
    "from shapes.construct_graph import *\n",
    "from shapes.shapes_generator import *\n",
    "from shapes.structural_graph import *\n",
    "from pattern_functions import *\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import seaborn as sb\n",
    "\n",
    "#sys.path.append('/Users/cdonnat/Desktop/structural_equivalents/')\n",
    "from distances_signature import *\n",
    "from heat_diffusion import *\n",
    "from clustering_via_distances import *\n",
    "import graph_tools \n",
    "#from stats.statistic_checks import *\n",
    "from purity import *\n",
    "from characteristic_functions import *\n",
    "from clustering_comparative_analysis import *\n",
    "from cluster_analysis import *\n",
    "from performance_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Define a set of simple two-layer backbone structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from geometric_graph import *\n",
    "from cell_block import *\n",
    "from base_model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape_list=[[\"fan\",6]]*10+[[\"star\",4]]*10+[[\"house\"]]*10\n",
    "training_set=[None]*200\n",
    "for b in range(200):\n",
    "    G,roles, index_roles, label_shape=build_lego_structure_from_structure(shape_list, start=0,plot=False,savefig=False,graph_type='nx.barabasi_albert_graph', graph_args=[4],save2text='/Users/cdonnat/Desktop/structural_equivalents/graphs/WS_3_06plus30_',add_node=30)\n",
    "    A=nx.adjacency_matrix(G).todense()\n",
    "    training_set[b]=geometric_representation()\n",
    "    training_set[b].initialize_from_adj(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<geometric_graph.geometric_representation instance at 0x1059cbfc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the geometric_graph implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from cell_block import *\n",
    "from geometric_graph import *\n",
    "from extraction import *\n",
    "\n",
    "class graphMRI():\n",
    "    def  __init__(self,training_set=None,n_cells=2,cell_params4training=None):\n",
    "        '''\n",
    "        specifies architecture +training paramters for the network\n",
    "        '''\n",
    "        self.training_set=training_set\n",
    "        self.final_error=None\n",
    "        self.n_cells=n_cells\n",
    "        self.training_inds=None\n",
    "        if cell_params4training==None:\n",
    "            self.cell_params4training={i+1:{'n_filters':100,'K':10, 'size_patch':2,'alpha':1,'sigma':1,\\\n",
    "                                'random_state':None, 'algorithm':'dictionary_learning'} for i in range(self.n_cells)}\n",
    "        else:\n",
    "            self.cell_params4training=cell_params4training\n",
    "        self.trained_cells={i+1: Cell() for i in range(self.n_cells)}\n",
    "        self.train_graph_seq=None\n",
    "    \n",
    "    def get_params_(self):\n",
    "        return {'training_set':self.training_set,'final_error':self.final_error,'n_cells':self.n_cells,\\\n",
    "                'cell_params4training':self.cell_params4training,'trained_cells':self.trained_cells,\\\n",
    "                'train_graph_seq':self.train_graph_seq,'training_inds':self.training_inds}\n",
    "    \n",
    "    def set_params_(self,dict_params):\n",
    "        valid_params=self.get_params_()\n",
    "        for k,v in dict_params.iteritems():\n",
    "            if k in valid_params:\n",
    "                setattr(self,k,v)\n",
    "            else:\n",
    "                print \"Warning, Invalid parameter %s.\"%k \n",
    "    \n",
    "    def train(training_set):\n",
    "        '''\n",
    "        function for training the MRI\n",
    "        INPUT:\n",
    "        =======================================================================\n",
    "        training_set     : a set of different graphs\n",
    "        \n",
    "        OUTPUT:\n",
    "        =======================================================================\n",
    "        self      : trained cells\n",
    "        '''\n",
    "        graph_seq={i: [] for i in range(self.n_cells+1)}\n",
    "        representations={i: [None]*len(training_set) for i in range(1,self.n_cells+1)}\n",
    "        inds=range(len(training_set))\n",
    "        np.random.shuffle(inds)\n",
    "        set_graphs=[training_set[i]for i in inds]\n",
    "        graph_seq[0]=set_graphs\n",
    "        self.training_inds=inds\n",
    "        self.training_set=traing_set\n",
    "        ### Start by extracting patches across different graphs and training the first layer\n",
    "        for c in range(1,self.n_cells+1):\n",
    "            train_features=extract_patches(graph_seq[c-1],n_patches,self.cell_params4training[c]['size_patch'])\n",
    "            chi_temp={i: characteristic_function(train_features[0][i],list(graph_seq[c-1][0].t))  for i in range(500)}\n",
    "            chi={i: list(chi_temp[i][:,1])+list(chi_temp[i][:,2]) for i in range(n_patches)}\n",
    "            train_features=np.array([chi[i] for i in range(n_patches)])\n",
    "            self.trained_cells[c].fit(train_features) ### trains stuff\n",
    "            graph_seq[c]=[]\n",
    "            threshold=self.cell_params4training[c]['thres']\n",
    "            #### create new graph set\n",
    "            for j in range(len(graphs)):\n",
    "                new_graph_geom_features=self.trained_cells[c].transform_features(graph_geom_features)\n",
    "                new_dist=self.trained_cells[c].transform_distance(new_graph_geom_features,graph_seq[c-1][j].distance,threshold)\n",
    "                new_graph=geometric_representation(distance=new_dist)\n",
    "                new_graph.adjacency_from_distance()\n",
    "                graph_geom_features=new_graph.compute_geometric_features()\n",
    "                graph_seq[c].append(new_graph)\n",
    "                ### extract eigenvalues of the the graph\n",
    "                lambdas,_,_=eig(rw_laplacian(new_graph.adjacency), left=True)\n",
    "                representation[c].append([list(new_graph_geom_features)+lambdas])\n",
    "        return self\n",
    "        \n",
    "    def get_representation(A):\n",
    "        '''\n",
    "        outputs the representation of the graph\n",
    "        INPUT:\n",
    "        =======================================================================\n",
    "        A      : adjacency matrix of the graph\n",
    "        \n",
    "        OUTPUT:\n",
    "        =======================================================================\n",
    "        output_features      : output features for the graphs\n",
    "        '''\n",
    "        representation=[]\n",
    "        for i in range(1,self.n_cells+1):\n",
    "            ### just output features based on each level's representation\n",
    "            print i\n",
    "        return False\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MRI=graphMRI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_seq={i: [] for i in range(MRI.n_cells+1)}\n",
    "representations={i: [None]*len(training_set) for i in range(1,MRI.n_cells+1)}\n",
    "inds=range(len(training_set))\n",
    "np.random.shuffle(inds)\n",
    "set_graphs=[training_set[i]for i in inds]\n",
    "graph_seq[0]=set_graphs\n",
    "MRI.training_inds=inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_params4training': {1: {'K': 10,\n",
       "   'algorithm': 'dictionary_learning',\n",
       "   'alpha': 1,\n",
       "   'n_filters': 100,\n",
       "   'random_state': None,\n",
       "   'sigma': 1,\n",
       "   'size_patch': 2},\n",
       "  2: {'K': 10,\n",
       "   'algorithm': 'dictionary_learning',\n",
       "   'alpha': 1,\n",
       "   'n_filters': 100,\n",
       "   'random_state': None,\n",
       "   'sigma': 1,\n",
       "   'size_patch': 2}},\n",
       " 'final_error': None,\n",
       " 'n_cells': 2,\n",
       " 'train_graph_seq': None,\n",
       " 'trained_cells': {1: <__main__.Cell instance at 0x11ce3c170>,\n",
       "  2: <__main__.Cell instance at 0x11ce3c248>},\n",
       " 'training_inds': [175,\n",
       "  111,\n",
       "  74,\n",
       "  122,\n",
       "  31,\n",
       "  79,\n",
       "  7,\n",
       "  194,\n",
       "  123,\n",
       "  75,\n",
       "  125,\n",
       "  107,\n",
       "  33,\n",
       "  162,\n",
       "  150,\n",
       "  102,\n",
       "  17,\n",
       "  8,\n",
       "  20,\n",
       "  177,\n",
       "  21,\n",
       "  129,\n",
       "  168,\n",
       "  19,\n",
       "  164,\n",
       "  153,\n",
       "  190,\n",
       "  87,\n",
       "  35,\n",
       "  11,\n",
       "  76,\n",
       "  158,\n",
       "  163,\n",
       "  108,\n",
       "  91,\n",
       "  124,\n",
       "  92,\n",
       "  42,\n",
       "  98,\n",
       "  167,\n",
       "  100,\n",
       "  192,\n",
       "  144,\n",
       "  24,\n",
       "  23,\n",
       "  198,\n",
       "  5,\n",
       "  196,\n",
       "  127,\n",
       "  30,\n",
       "  170,\n",
       "  68,\n",
       "  120,\n",
       "  12,\n",
       "  137,\n",
       "  46,\n",
       "  0,\n",
       "  93,\n",
       "  88,\n",
       "  41,\n",
       "  97,\n",
       "  172,\n",
       "  155,\n",
       "  134,\n",
       "  2,\n",
       "  65,\n",
       "  142,\n",
       "  44,\n",
       "  16,\n",
       "  106,\n",
       "  48,\n",
       "  139,\n",
       "  83,\n",
       "  169,\n",
       "  57,\n",
       "  185,\n",
       "  71,\n",
       "  1,\n",
       "  113,\n",
       "  147,\n",
       "  59,\n",
       "  78,\n",
       "  63,\n",
       "  55,\n",
       "  183,\n",
       "  67,\n",
       "  52,\n",
       "  133,\n",
       "  105,\n",
       "  161,\n",
       "  26,\n",
       "  117,\n",
       "  103,\n",
       "  85,\n",
       "  143,\n",
       "  181,\n",
       "  10,\n",
       "  179,\n",
       "  64,\n",
       "  54,\n",
       "  184,\n",
       "  193,\n",
       "  128,\n",
       "  99,\n",
       "  13,\n",
       "  130,\n",
       "  3,\n",
       "  51,\n",
       "  62,\n",
       "  36,\n",
       "  116,\n",
       "  178,\n",
       "  109,\n",
       "  73,\n",
       "  115,\n",
       "  187,\n",
       "  157,\n",
       "  49,\n",
       "  148,\n",
       "  80,\n",
       "  126,\n",
       "  60,\n",
       "  82,\n",
       "  110,\n",
       "  165,\n",
       "  138,\n",
       "  40,\n",
       "  145,\n",
       "  152,\n",
       "  45,\n",
       "  84,\n",
       "  151,\n",
       "  70,\n",
       "  18,\n",
       "  119,\n",
       "  28,\n",
       "  154,\n",
       "  160,\n",
       "  189,\n",
       "  112,\n",
       "  199,\n",
       "  136,\n",
       "  101,\n",
       "  9,\n",
       "  25,\n",
       "  47,\n",
       "  22,\n",
       "  146,\n",
       "  39,\n",
       "  37,\n",
       "  32,\n",
       "  53,\n",
       "  89,\n",
       "  188,\n",
       "  95,\n",
       "  121,\n",
       "  96,\n",
       "  180,\n",
       "  34,\n",
       "  14,\n",
       "  81,\n",
       "  43,\n",
       "  135,\n",
       "  77,\n",
       "  131,\n",
       "  171,\n",
       "  61,\n",
       "  58,\n",
       "  15,\n",
       "  6,\n",
       "  191,\n",
       "  4,\n",
       "  166,\n",
       "  140,\n",
       "  149,\n",
       "  69,\n",
       "  86,\n",
       "  195,\n",
       "  29,\n",
       "  118,\n",
       "  94,\n",
       "  114,\n",
       "  173,\n",
       "  156,\n",
       "  104,\n",
       "  176,\n",
       "  27,\n",
       "  197,\n",
       "  50,\n",
       "  182,\n",
       "  159,\n",
       "  174,\n",
       "  141,\n",
       "  90,\n",
       "  38,\n",
       "  72,\n",
       "  132,\n",
       "  56,\n",
       "  186,\n",
       "  66],\n",
       " 'training_set': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRI.get_params_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "train_features=extract_patches(graph_seq[i-1],500,MRI.cell_params4training[i]['size_patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c=1\n",
    "chi_temp={i: characteristic_function(train_features[i],list(graph_seq[c-1][0].t)) for i in range(500)}\n",
    "chi={i: list(chi_temp[i][:,1])+list(chi_temp[i][:,2]) for i in range(500)}\n",
    "train_features2=np.array([chi[i] for i in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Cell instance at 0x11ce3c170>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(MRI.trained_cells[c]).fit(train_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testy=MRI.trained_cells[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_seq[c]=[]\n",
    "threshold=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'gg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-05d306df4fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_geometric_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/cdonnat/Dropbox/Pattern-analysis/pattern_detection/geometric_graph.py\u001b[0m in \u001b[0;36mcompute_geometric_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m### induce spatial invariance by polling over neighbors or over the patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             chi_new={i:np.sum([chi[ind]*np.exp(-self.distance.loc[i,ind]/self.sigma) for ind in List_neigh[i].index],0)/np.sum(np.exp(-self.distance.loc[i,ind]/gg.sigma) for ind in list(List_neigh[i].index.values)+[i])\\\n\u001b[0;32m---> 68\u001b[0;31m              for i in range(self.n_nodes)} ## sum or mean?\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mchi_new\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mchi_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m           \u001b[0;31m### to get n_nodes x dim_feature space...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/Dropbox/Pattern-analysis/pattern_detection/geometric_graph.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((i,))\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m### induce spatial invariance by polling over neighbors or over the patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             chi_new={i:np.sum([chi[ind]*np.exp(-self.distance.loc[i,ind]/self.sigma) for ind in List_neigh[i].index],0)/np.sum(np.exp(-self.distance.loc[i,ind]/gg.sigma) for ind in list(List_neigh[i].index.values)+[i])\\\n\u001b[0;32m---> 68\u001b[0;31m              for i in range(self.n_nodes)} ## sum or mean?\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mchi_new\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mchi_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m           \u001b[0;31m### to get n_nodes x dim_feature space...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1819\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/Dropbox/Pattern-analysis/pattern_detection/geometric_graph.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((ind,))\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m### PCA representation? But that would have to be carried out on the entrie datset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m### induce spatial invariance by polling over neighbors or over the patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             chi_new={i:np.sum([chi[ind]*np.exp(-self.distance.loc[i,ind]/self.sigma) for ind in List_neigh[i].index],0)/np.sum(np.exp(-self.distance.loc[i,ind]/gg.sigma) for ind in list(List_neigh[i].index.values)+[i])\\\n\u001b[0m\u001b[1;32m     68\u001b[0m              for i in range(self.n_nodes)} ## sum or mean?\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'gg' is not defined"
     ]
    }
   ],
   "source": [
    "training_set[0].compute_geometric_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_graph_geom_features=MRI.trained_cells[c].transform_features(G.geom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G=geometric_representation(distance=training_set[0].distance,adjacency=training_set[0].adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G.initialize_from_adj(training_set[0].adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.compute_geometric_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-08935c285f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_graph_geom_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMRI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_cells\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeom_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-5b3fc836cf59>\u001b[0m in \u001b[0;36mtransform_features\u001b[0;34m(self, new_features)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mnew_features_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_tool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_features_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-5b3fc836cf59>\u001b[0m in \u001b[0;36mprediction_tool\u001b[0;34m(new_input)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mdict_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDictionaryLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_patterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mprediction_tool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mdict_patterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_tool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_patterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;31m# XXX : kwargs is not documented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cdonnat/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "new_graph_geom_features=MRI.trained_cells[c].transform_features(training_set[j].geom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isnan(new_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12604194, -0.12591289, -0.12552887, ..., -0.02088432,\n",
       "        -0.02032695, -0.01979812],\n",
       "       [-0.15799792, -0.15637687, -0.15158629, ..., -0.11409841,\n",
       "        -0.11495493, -0.11349926],\n",
       "       [-0.15007849, -0.14941915, -0.1474527 , ...,  0.04431573,\n",
       "         0.03962536,  0.03460123],\n",
       "       ..., \n",
       "       [ 0.01272911, -0.03662893,  0.04409591, ..., -0.08387186,\n",
       "         0.0130269 ,  0.01055019],\n",
       "       [-0.08511005, -0.0622547 ,  0.00868761, ..., -0.02842517,\n",
       "         0.01003064,  0.07422859],\n",
       "       [ 0.01057132, -0.00382855, -0.01170376, ..., -0.01247158,\n",
       "        -0.07613994, -0.03438622]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### create new graph set\n",
    "for j in range(len(training_set)):\n",
    "    G=training_set[j]\n",
    "    new_graph_geom_features=self.trained_cells[c].transform_features(training_set[j].geom_features)\n",
    "    new_dist=self.trained_cells[c].transform_distance(new_graph_geom_features,graph_seq[c-1][j].distance,threshold)\n",
    "    new_graph=geometric_representation(distance=new_dist)\n",
    "    new_graph.adjacency_from_distance()\n",
    "    graph_geom_features=new_graph.compute_geometric_features()\n",
    "    graph_seq[c].append(new_graph)\n",
    "    ### extract eigenvalues of the the graph\n",
    "    lambdas,_,_=eig(rw_laplacian(new_graph.adjacency), left=True)\n",
    "    representation[c].append([list(new_graph_geom_features)+lambdas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(np.isinf(G.geom_features )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features2=np.array([chi[i] for i in range(500)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_seq[c]=[]\n",
    "threshold=0.01\n",
    "#### create new graph set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi_temp={i: characteristic_function(train_features[0][i],graph_seq[i-1][0].t) for i in range(500)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    tset[j]training_set[j].\n",
    "    print len(train_features[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_patches(set_graphs,n_patches,K=2):\n",
    "   \n",
    "    ### assignment\n",
    "    nb_per_graph=np.random.multinomial(n_patches, [1.0/len(set_graphs)]*len(set_graphs))\n",
    "    patch=[]\n",
    "    nodes2patches={}\n",
    "    it=0\n",
    "    #print nb_per_graph\n",
    "    for it in range(len(set_graphs)):\n",
    "        if it%20==0: print it\n",
    "        G=set_graphs[it]\n",
    "        A=G.adjacency\n",
    "        neighbor_lookup={i: np.where(np.sum([np.linalg.matrix_power(A,kk)[i,:] for kk in range(1,K+1)],0)>0)[1] for i in range(A.shape[0])} \n",
    "        nodes2patches[i]=neighbor_lookup\n",
    "        nodes=np.random.choice(range(A.shape[0]),nb_per_graph[it],replace=False)\n",
    "        for n in nodes:\n",
    "            ind_interest=neighbor_lookup[i]\n",
    "            A_tilde=A[:,ind_interest][ind_interest,:]\n",
    "            lambdas, chi, vr = eig(rw_laplacian(A_tilde), left=True)\n",
    "            lambdas=np.real(lambdas)\n",
    "            zero=np.argmin(lambdas)\n",
    "            patch+=[1.0/np.sum(chi[:,zero])*chi[:,zero]]\n",
    "\n",
    "    return patch,nodes2patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds=range(len(graph_seq[i-1]))\n",
    "np.random.shuffle(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_seq[i-1]=graph_seq[i-1][inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.heatmap(gg.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "List_patterns,List_neigh=cut_laplacian(gg.adjacency,gg.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi_temp={i: characteristic_function(List_patterns[i],list(gg.t)) for i in range(gg.n_nodes)}\n",
    "chi={i: np.array(list(chi_temp[i][:,1])+list(chi_temp[i][:,2])) for i in range(gg.n_nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi_new=pd.DataFrame.from_dict( chi_new).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.heatmap(chi_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum([chi[ind]*np.exp(-gg.distance.loc[i,ind]/gg.sigma) for ind in list(List_neigh[i].index.values)+[i]],0)/np.sum(np.exp(-gg.distance.loc[i,ind]/gg.sigma) for ind in list(List_neigh[i].index.values)+[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(chi_new[0][:150],chi_new[0][150:])\n",
    "plt.plot(chi[0][:150],chi[0][150:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg.get_params_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "List_patterns,List_neigh=cut_laplacian(gg.adjacency,gg.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D=pd.DataFrame([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=A.shape[0]    ##nb of nodes\n",
    "### create lookup of negbors index:\n",
    "List_patterns={}\n",
    "k=1\n",
    "List_neighborhoods={}\n",
    "neighbor_lookup={i: np.where(np.sum([np.linalg.matrix_power(A,kk)[i,:] for kk in range(1,k+1)],0)>0)[1] for i in range(N)} ##nodes reached in at most #k hops\n",
    "for i in range(N):  ##for each node\n",
    "\n",
    "    ind_interest=list(set(neighbor_lookup[i]+[i]))\n",
    "    print ind_interest\n",
    "    A_tilde=A[:,ind_interest][ind_interest,:]\n",
    "    ### Compute the associated RW laplacian and its first eigenvector\n",
    "    lambdas, chi, vr = eig(rw_laplacian(A_tilde), left=True)\n",
    "    lambdas=np.real(lambdas)\n",
    "    zero=np.argmin(lambdas)\n",
    "    pi=chi[:,zero]  ## keep only the first eigenvector\n",
    "    List_patterns[i]=1.0/np.sum(chi[:,zero])*chi[:,zero] ### normalize so that we have a probability distribution\n",
    "\n",
    "    List_neighborhoods[i]=pd.DataFrame(A_tilde, index=ind_interest,columns=ind_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(len(training_set)):\n",
    "    G=training_set[j]\n",
    "    List_patterns,List_neigh=cut_laplacian(G.adjacency,G.patch_size)\n",
    "    chi_temp={i: characteristic_function(List_patterns[i],list(G.t)) for i in range(G.n_nodes)}\n",
    "    chi={i: np.array(list(chi_temp[i][:,1])+list(chi_temp[i][:,2])) for i in range(G.n_nodes)}\n",
    "    ### PCA representation? But that would have to be carried out on the entrie datset\n",
    "    ### induce spatial invariance by polling over neighbors or over the patch\n",
    "    chi_new={i:np.sum([chi[ind]*np.exp(-G.distance.loc[i,ind]/G.sigma) for ind in List_neigh[i].index],0)/np.sum(np.exp(-G.distance.loc[i,ind]/G.sigma) for ind in list(List_neigh[i].index.values)+[i])\\\n",
    "     for i in range(G.n_nodes)} ## sum or mean?\n",
    "\n",
    "    chi_new=pd.DataFrame.from_dict( chi_new).T           ### to get n_nodes x dim_feature space...\n",
    "    G.geom_features=chi_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(len(training_set)):\n",
    "    graph_geom_features=training_set[j].geom_features\n",
    "    new_graph_geom_features=MRI.trained_cells[c].transform_features(graph_geom_features)\n",
    "    new_dist=MRI.trained_cells[c].transform_distance(new_graph_geom_features,graph_seq[c-1][j].distance,threshold)\n",
    "    new_graph=geometric_representation(distance=new_dist)\n",
    "    new_graph.adjacency_from_distance()\n",
    "    graph_geom_features=new_graph.compute_geometric_features()\n",
    "    graph_seq[c].append(new_graph)\n",
    "    ### extract eigenvalues of the the graph\n",
    "    lambdas,_,_=eig(rw_laplacian(new_graph.adjacency), left=True)\n",
    "    representation[c].append([list(new_graph_geom_features)+lambdas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set[0].geom_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MRI.trained_cells[c].transform_features(graph_geom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gg.geom_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start to train the different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.log(2*math.pi*sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class geometric_representation():\n",
    "    def __init__(self,n_nodes=None,adjacency=None,distance=None,geom_features=None,patch_size=1,sigma=1,representation='RW_laplacian',t=np.arange(0,150,1)):\n",
    "        self.n_nodes=n_nodes\n",
    "        self.adjacency=adjacency\n",
    "        self.distance=distance\n",
    "        self.geom_features=geom_features\n",
    "        self.patch_size=patch_size\n",
    "        self.sigma=sigma\n",
    "        self.representation=representation\n",
    "        self.t=t\n",
    "        \n",
    "    def set_graph_(self, D, A=None):\n",
    "        ### need to handle the case where A is a sparse matrix\n",
    "        self.n_nodes=D.shape[0]\n",
    "        self.adjacency=A \n",
    "        self.distance=pd.DataFrame(D)\n",
    "        return self\n",
    "    \n",
    "    def initialize_from_adj(self,A):\n",
    "        self.n_nodes=A.shape[0]\n",
    "        self.adjacency=A \n",
    "        dist=-2*self.sigma*np.log(1.0/np.sqrt(2*math.pi*self.sigma)*(A+0.001))\n",
    "        self.distance=pd.DataFrame(dist)\n",
    "    \n",
    "    def set_params_(self,dict_params):\n",
    "        valid_params=self.get_params_()\n",
    "        for k,v in dict_params.iteritems():\n",
    "            if k in valid_params:\n",
    "                setattr(self,k,v)\n",
    "            else:\n",
    "                print \"Warning, Invalid parameter %s.\"%k \n",
    "    \n",
    "    def get_params_(self):\n",
    "        return {'n_nodes':self.n_nodes,'adjacency': self.adjacency,'distance':self.distance,\\\n",
    "        'geom_features':self.geom_features,'patch_size':self.patch_size,\\\n",
    "        'representation':self.representation}\n",
    "    \n",
    "    def adjacency_from_distance(self):\n",
    "        adj=np.exp(-self.D.values/(2.0*self.sigma))\n",
    "        np.fill_diagonal(adj,0)\n",
    "        self.adjacency=adj\n",
    "        return self\n",
    "    \n",
    "    def compute_geometric_features(self):\n",
    "        if self.representation=='RW_laplacian':\n",
    "            List_patterns,List_neigh=cut_laplacian(self.adjacency,self.patch_size)\n",
    "            chi_temp={i: characteristic_function(List_patterns[i],list(self.t)) for i in range(self.n_nodes)}\n",
    "            chi={i: np.array(list(chi_temp[i][:,1])+list(chi_temp[i][:,2])) for i in range(self.n_nodes)}\n",
    "            ### PCA representation? But that would have to be carried out on the entrie datset\n",
    "            ### induce spatial invariance by polling over neighbors or over the patch\n",
    "            chi_new={i:np.sum([chi[ind]*np.exp(-self.distance.loc[i,ind]/self.sigma) for ind in List_neigh[i].index],0)/np.sum(np.exp(-self.distance.loc[i,ind]/self.sigma) for ind in list(List_neigh[i].index.values)+[i])\\\n",
    "             for i in range(self.n_nodes)} ## sum or mean?\n",
    "           \n",
    "            chi_new=pd.DataFrame.from_dict( chi_new).T           ### to get n_nodes x dim_feature space...\n",
    "            self.geom_features=chi_new\n",
    "        elif self.representation=='fisher_vector':\n",
    "            print 'yup'\n",
    "        else:\n",
    "            print 'Representation type not recognized/implemented yet. Better luck next time.'\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class graphMRI():\n",
    "    def  __init__(self):\n",
    "        ''' specifies architecture +training paramters for the network\n",
    "        '''\n",
    "        self.training_set=None\n",
    "        self.final_error=None \n",
    "        self.n_cells=2\n",
    "        self.cell_params4training={i+1:{'n_filters':100,'K':10, 'size_patch':2, 'alpha':1,'sigma':1,\\\n",
    "        'random_state':None,'algorithm':'dictionary_learning'} for i in range(self.n_cells)}\n",
    "        self.trained_cells={i+1:{Cell()} for i in range(self.n_cells)}\n",
    "        self.train_graph_seq=None\n",
    "        self.train_inds=None\n",
    "    \n",
    "    \n",
    "    def train(training_set):\n",
    "        '''\n",
    "        function for training the MRI\n",
    "        INPUT:\n",
    "        =======================================================================\n",
    "        training_set     : a set of different graphs\n",
    "        \n",
    "        OUTPUT:\n",
    "        =======================================================================\n",
    "        self      : trained cells\n",
    "        '''\n",
    "        graph_seq={i: [] for i in range(self.n_cells+1)}\n",
    "        representations={i: [None]*len(training_set) for i in range(1,self.n_cells+1)}\n",
    "        inds=range(len(training_set))\n",
    "        np.random.shuffle(inds)\n",
    "        set_graphs=[training_set[i]for i in inds]\n",
    "        graph_seq[0]=set_graphs\n",
    "        ### Start by extracting patches across different graphs and training the first layer\n",
    "        for i in range(1,self.n_cells+1):\n",
    "            train_features=extract_patches(graph_seq[i-1],n_patches,self.cell_params4training[i]['size_patch'])\n",
    "            self.trained_cells[i].fit(train_features) ### trains stuff\n",
    "            graph_seq[i]=[]\n",
    "            threshold=self.cell_params4training[i]['thres']\n",
    "            #### create new graph set\n",
    "            for j in range(len(graphs)):\n",
    "                new_graph_geom_features=self.trained_cells[i].transform_features(graph_geom_features)\n",
    "                new_dist=self.trained_cells[i].transform_distance(new_graph_geom_features,graph_seq[i-1][j].distance,threshold)\n",
    "                new_graph=geometric_representation(distance=new_dist)\n",
    "                new_graph.adjacency_from_distance()\n",
    "                graph_geom_features=new_graph.compute_geometric_features()\n",
    "                graph_seq[i].append(new_graph)\n",
    "                ### extract eigenvalues of the the graph\n",
    "                lambdas,_,_=eig(rw_laplacian(new_graph.adjacency), left=True)\n",
    "                representation[i].append([list(new_graph_geom_features)+lambdas])\n",
    "        return self\n",
    "        \n",
    "    def get_representation(A):\n",
    "        '''\n",
    "        outputs the representation of the graph\n",
    "        INPUT:\n",
    "        =======================================================================\n",
    "        A      : adjacency matrix of the graph\n",
    "        \n",
    "        OUTPUT:\n",
    "        =======================================================================\n",
    "        output_features      : output features for the graphs\n",
    "        '''\n",
    "        representation=[]\n",
    "        for i in range(1,self.n_cells+1):\n",
    "            ### just output features based on each level's representation\n",
    "            print i\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Cell():\n",
    "    def __init__(self, n_filters=100,K=10, alpha=1,sigma=1,size_patch=2,random_state=None,algorithm='dictionary_learning'):\n",
    "        self.filters={i: np.random.rand(K) for i in range(n_filters)}\n",
    "        self.n_filters=n_filters\n",
    "        self.K=K\n",
    "        self.size_patch=size_patch\n",
    "        self.random_state=random_state\n",
    "        self.sigma=sigma\n",
    "        self.algorithm=algorithm\n",
    "        self.n_samples=0\n",
    "        self.alpha=1\n",
    "        self.eta=0.01\n",
    "        self.pooled={}\n",
    "        self.bbox=None\n",
    "    \n",
    "    def set_params_(self, dictionary_params):\n",
    "        valid_params=self.get_params()\n",
    "        for k,v in dictionary_params.iteritems():\n",
    "            if k in valid_params:\n",
    "                setattr(self,k,v)\n",
    "            else:\n",
    "                print \"Warning, Invalid parameter %s.\"%k \n",
    "        return self\n",
    "            \n",
    "    \n",
    "    def get_params_(self):\n",
    "        return {'filters':self.filters,'n_filters':self.n_filters,'K':self.K,'random_state':self.random_state,\\\n",
    "        'sigma':self.sigma,'algorithm':self.algorithm,'n_samples':self.n_samples,'alpha':self.alpha,\\\n",
    "        'eta':self.eta,'pooled':self.pooled,'size_patch':self.size_patch,'bbox_alg':self.bbox}\n",
    "        \n",
    "    def fit(self, features):\n",
    "        rnd=check_random_state(self.random_state)\n",
    "        n_samples=len(features)\n",
    "        self.n_samples=n_samples\n",
    "        ### Permute the training graphs (because you never know)\n",
    "        inds=rnd.permutation(n_samples)\n",
    "        train_features=features[inds,:] ### learning dictionary\n",
    "        \n",
    "        if self.algorithm=='dictionary_learning':\n",
    "            dict_patterns=DictionaryLearning(n_components=self.n_filters,alpha=self.alpha )\n",
    "            self.bbox=dict_patterns\n",
    "            self.train_output=dict_patterns.fit_transform(train_features)\n",
    "            def prediction_tool(new_input): return dict_patterns.transform(new_input)\n",
    "            self.prediction_tool=prediction_tool\n",
    "            self.filters=dict_patterns.components_\n",
    "        elif self.algorithm=='mixture':\n",
    "            ### fuzzy clustering\n",
    "            ### Implement sparse clustering?\n",
    "            cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(train_features.T, self.n_filters, m=2, error=0.005, maxiter=1000, init=None)\n",
    "            self.filters=cntr.T\n",
    "            self.train_output=u.T\n",
    "            def prediction_tool(new_input): return fuzz.cmeans_predict(new_input.T, cntr, m=2, error=0.005, maxiter=1000)\n",
    "            self.prediction_tool=prediction_tool\n",
    "        else:\n",
    "            print \"algorithm not implemented yet\"\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform_features(self, new_features):\n",
    "        new_features_output=self.prediction_tool(new_features)\n",
    "        return new_features_output\n",
    "        \n",
    "        \n",
    "    def transform_distance(self, feature_output,distance,thres):\n",
    "        ### get appropriate kernel:\n",
    "        K1=(feature_output).dot(feature_output.T)\n",
    "        K=np.exp(-(distance+K1)*1.0/self.sigma)\n",
    "        new_distance=np.multiply(K,distance)\n",
    "        ## threshold new distance\n",
    "        def prune(x,eta): return x*(x>eta)\n",
    "        ### vectorize \n",
    "        vprune=np.vectorize(prune)\n",
    "        new_distance=vprune(new_distance,self.eta)\n",
    "        ### pool nodes that are at distance 0 \n",
    "        ### take as the \"centroids\" the nodes that have maximum correlation with filter\n",
    "        ### start by ranking the nodes based on their entropy:\n",
    "        def entropy(x): return -x.dot(np.log(x))\n",
    "        entropy_nodes=np.apply_along_axis(entropy, 0,feature_output)\n",
    "        inds=np.argsort(entropy_nodes)\n",
    "        for i in range(1,new_distance.shape[1]):\n",
    "            indd=inds[-i]\n",
    "            nodes_to_pool=np.where(new_distance[indd,:]==0)[0]\n",
    "            if len(nodes_to_pool)>0:\n",
    "                z=np.array([False]*new_distance.shape[1])\n",
    "                z[nodes_to_pool]=True\n",
    "                d_tilde=np.mean(new_distance[indd,nodes_to_pool],1)\n",
    "                new_distance[indd,:]=d_tilde\n",
    "                new_distance=new_distance.compress(np.logical_not(z), axis=1)\n",
    "                new_distance=vprune(new_distance,self.eta)\n",
    "                new_feature_output=feature_output.compress(np.logical_not(z), axis=1)\n",
    "                self.pooled[indd]=nodes_to_pool\n",
    "        \n",
    "        return new_feature_output,new_distance\n",
    "        \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_patches(set_graphs,n_patches,K=2):\n",
    "   \n",
    "    ### assignment\n",
    "    nb_per_graph=np.random.multinomial(n_patches, [1.0/len(set_graphs)]*len(set_graphs))\n",
    "    patch=[]\n",
    "    it=0\n",
    "    for it in range(len(set_graphs)):\n",
    "        G=set_graphs[it]\n",
    "        A=G.adjacency\n",
    "        neighbor_lookup={i: np.where(np.sum([np.linalg.matrix_power(A,kk)[i,:] for kk in range(1,K+1)],0)>0)[1] for i in range(A.shape[0])} \n",
    "        nodes=np.random.choice(range(A.shape[0]),nb_per_graph[it])\n",
    "        for n in nodes:\n",
    "            ind_interest=neighbor_lookup[i]\n",
    "            A_tilde=A[:,ind_interest][ind_interest,:]\n",
    "            lambdas, chi, vr = eig(rw_laplacian(A_tilde), left=True)\n",
    "            lambdas=np.real(lambdas)\n",
    "            zero=np.argmin(lambdas)\n",
    "            patch+=[1.0/np.sum(chi[:,zero])*chi[:,zero]]\n",
    "        it+=1\n",
    "    return patch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
