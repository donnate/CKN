{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%matplolib` not found.\n"
     ]
    }
   ],
   "source": [
    "%matplolib inline\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import os, time, collections, shutil\n",
    "\n",
    "\n",
    "#NFEATURES = 28**2\n",
    "#NCLASSES = 10\n",
    "\n",
    "\n",
    "# Common methods for all models\n",
    "\n",
    "\n",
    "class base_model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.regularizers = []\n",
    "    \n",
    "    # High-level interface which runs the constructed computational graph.\n",
    "    \n",
    "    def predict(self, data, labels=None, sess=None):\n",
    "        loss = 0\n",
    "        size = data.shape[0]\n",
    "        predictions = np.empty(size)\n",
    "        sess = self._get_session(sess)\n",
    "        for begin in range(0, size, self.batch_size):\n",
    "            end = begin + self.batch_size\n",
    "            end = min([end, size])\n",
    "            \n",
    "            batch_data = np.zeros((self.batch_size, data.shape[1]))\n",
    "            tmp_data = data[begin:end,:]\n",
    "            if type(tmp_data) is not np.ndarray:\n",
    "                tmp_data = tmp_data.toarray()  # convert sparse matrices\n",
    "            batch_data[:end-begin] = tmp_data\n",
    "            feed_dict = {self.ph_data: batch_data, self.ph_dropout: 1}\n",
    "            \n",
    "            # Compute loss if labels are given.\n",
    "            if labels is not None:\n",
    "                batch_labels = np.zeros(self.batch_size)\n",
    "                batch_labels[:end-begin] = labels[begin:end]\n",
    "                feed_dict[self.ph_labels] = batch_labels\n",
    "                batch_pred, batch_loss = sess.run([self.op_prediction, self.op_loss], feed_dict)\n",
    "                loss += batch_loss\n",
    "            else:\n",
    "                batch_pred = sess.run(self.op_prediction, feed_dict)\n",
    "            \n",
    "            predictions[begin:end] = batch_pred[:end-begin]\n",
    "            \n",
    "        if labels is not None:\n",
    "            return predictions, loss * self.batch_size / size\n",
    "        else:\n",
    "            return predictions\n",
    "        \n",
    "    def evaluate(self, data, labels, sess=None):\n",
    "        \"\"\"\n",
    "        Runs one evaluation against the full epoch of data.\n",
    "        Return the precision and the number of correct predictions.\n",
    "        Batch evaluation saves memory and enables this to run on smaller GPUs.\n",
    "\n",
    "        sess: the session in which the model has been trained.\n",
    "        op: the Tensor that returns the number of correct predictions.\n",
    "        data: size N x M\n",
    "            N: number of signals (samples)\n",
    "            M: number of vertices (features)\n",
    "        labels: size N\n",
    "            N: number of signals (samples)\n",
    "        \"\"\"\n",
    "        t_process, t_wall = time.process_time(), time.time()\n",
    "        predictions, loss = self.predict(data, labels, sess)\n",
    "        #print(predictions)\n",
    "        ncorrects = sum(predictions == labels)\n",
    "        accuracy = 100 * sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        f1 = 100 * sklearn.metrics.f1_score(labels, predictions, average='weighted')\n",
    "        string = 'accuracy: {:.2f} ({:d} / {:d}), f1 (weighted): {:.2f}, loss: {:.2e}'.format(\n",
    "                accuracy, ncorrects, len(labels), f1, loss)\n",
    "        if sess is None:\n",
    "            string += '\\ntime: {:.0f}s (wall {:.0f}s)'.format(time.process_time()-t_process, time.time()-t_wall)\n",
    "        return string, accuracy, f1, loss\n",
    "\n",
    "    def fit(self, train_data, train_labels, val_data, val_labels):\n",
    "        t_process, t_wall = time.process_time(), time.time()\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        shutil.rmtree(self._get_path('summaries'), ignore_errors=True)\n",
    "        writer = tf.summary.FileWriter(self._get_path('summaries'), self.graph)\n",
    "        shutil.rmtree(self._get_path('checkpoints'), ignore_errors=True)\n",
    "        os.makedirs(self._get_path('checkpoints'))\n",
    "        path = os.path.join(self._get_path('checkpoints'), 'model')\n",
    "        sess.run(self.op_init)\n",
    "\n",
    "        # Training.\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        indices = collections.deque()\n",
    "        num_steps = int(self.num_epochs * train_data.shape[0] / self.batch_size)\n",
    "        for step in range(1, num_steps+1):\n",
    "\n",
    "            # Be sure to have used all the samples before using one a second time.\n",
    "            if len(indices) < self.batch_size:\n",
    "                indices.extend(np.random.permutation(train_data.shape[0]))\n",
    "            idx = [indices.popleft() for i in range(self.batch_size)]\n",
    "\n",
    "            batch_data, batch_labels = train_data[idx,:], train_labels[idx]\n",
    "            if type(batch_data) is not np.ndarray:\n",
    "                batch_data = batch_data.toarray()  # convert sparse matrices\n",
    "            feed_dict = {self.ph_data: batch_data, self.ph_labels: batch_labels, self.ph_dropout: self.dropout}\n",
    "            learning_rate, loss_average = sess.run([self.op_train, self.op_loss_average], feed_dict)\n",
    "\n",
    "            # Periodical evaluation of the model.\n",
    "            if step % self.eval_frequency == 0 or step == num_steps:\n",
    "                epoch = step * self.batch_size / train_data.shape[0]\n",
    "                print('step {} / {} (epoch {:.2f} / {}):'.format(step, num_steps, epoch, self.num_epochs))\n",
    "                print('  learning_rate = {:.2e}, loss_average = {:.2e}'.format(learning_rate, loss_average))\n",
    "                string, accuracy, f1, loss = self.evaluate(val_data, val_labels, sess)\n",
    "                accuracies.append(accuracy)\n",
    "                losses.append(loss)\n",
    "                print('  validation {}'.format(string))\n",
    "                print('  time: {:.0f}s (wall {:.0f}s)'.format(time.process_time()-t_process, time.time()-t_wall))\n",
    "\n",
    "                # Summaries for TensorBoard.\n",
    "                summary = tf.Summary()\n",
    "                summary.ParseFromString(sess.run(self.op_summary, feed_dict))\n",
    "                summary.value.add(tag='validation/accuracy', simple_value=accuracy)\n",
    "                summary.value.add(tag='validation/f1', simple_value=f1)\n",
    "                summary.value.add(tag='validation/loss', simple_value=loss)\n",
    "                writer.add_summary(summary, step)\n",
    "                \n",
    "                # Save model parameters (for evaluation).\n",
    "                self.op_saver.save(sess, path, global_step=step)\n",
    "\n",
    "        print('validation accuracy: peak = {:.2f}, mean = {:.2f}'.format(max(accuracies), np.mean(accuracies[-10:])))\n",
    "        writer.close()\n",
    "        sess.close()\n",
    "        \n",
    "        t_step = (time.time() - t_wall) / num_steps\n",
    "        return accuracies, losses, t_step\n",
    "\n",
    "    def get_var(self, name):\n",
    "        sess = self._get_session()\n",
    "        var = self.graph.get_tensor_by_name(name + ':0')\n",
    "        val = sess.run(var)\n",
    "        sess.close()\n",
    "        return val\n",
    "\n",
    "    # Methods to construct the computational graph.\n",
    "    \n",
    "    def build_graph(self, M_0):\n",
    "        \"\"\"Build the computational graph of the model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # Inputs.\n",
    "            with tf.name_scope('inputs'):\n",
    "                self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n",
    "                self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
    "                self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "\n",
    "            # Model.\n",
    "            op_logits = self.inference(self.ph_data, self.ph_dropout)\n",
    "            self.op_loss, self.op_loss_average = self.loss(op_logits, self.ph_labels, self.regularization)\n",
    "            self.op_train = self.training(self.op_loss, self.learning_rate,\n",
    "                    self.decay_steps, self.decay_rate, self.momentum)\n",
    "            self.op_prediction = self.prediction(op_logits)\n",
    "\n",
    "            # Initialize variables, i.e. weights and biases.\n",
    "            self.op_init = tf.global_variables_initializer()\n",
    "            \n",
    "            # Summaries for TensorBoard and Save for model parameters.\n",
    "            self.op_summary = tf.summary.merge_all()\n",
    "            self.op_saver = tf.train.Saver(max_to_keep=5)\n",
    "        \n",
    "        self.graph.finalize()\n",
    "    \n",
    "    def inference(self, data, dropout):\n",
    "        \"\"\"\n",
    "        It builds the model, i.e. the computational graph, as far as\n",
    "        is required for running the network forward to make predictions,\n",
    "        i.e. return logits given raw data.\n",
    "\n",
    "        data: size N x M\n",
    "            N: number of signals (samples)\n",
    "            M: number of vertices (features)\n",
    "        training: we may want to discriminate the two, e.g. for dropout.\n",
    "            True: the model is built for training.\n",
    "            False: the model is built for evaluation.\n",
    "        \"\"\"\n",
    "        # TODO: optimizations for sparse data\n",
    "        logits = self._inference(data, dropout)\n",
    "        return logits\n",
    "    \n",
    "    def probabilities(self, logits):\n",
    "        \"\"\"Return the probability of a sample to belong to each class.\"\"\"\n",
    "        with tf.name_scope('probabilities'):\n",
    "            probabilities = tf.nn.softmax(logits)\n",
    "            return probabilities\n",
    "\n",
    "    def prediction(self, logits):\n",
    "        \"\"\"Return the predicted classes.\"\"\"\n",
    "        with tf.name_scope('prediction'):\n",
    "            prediction = tf.argmax(logits, axis=1)\n",
    "            return prediction\n",
    "\n",
    "    def loss(self, logits, labels, regularization):\n",
    "        \"\"\"Adds to the inference model the layers required to generate loss.\"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                labels = tf.to_int64(labels)\n",
    "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "                cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "            with tf.name_scope('regularization'):\n",
    "                regularization *= tf.add_n(self.regularizers)\n",
    "            loss = cross_entropy + regularization\n",
    "            \n",
    "            # Summaries for TensorBoard.\n",
    "            tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
    "            tf.summary.scalar('loss/regularization', regularization)\n",
    "            tf.summary.scalar('loss/total', loss)\n",
    "            with tf.name_scope('averages'):\n",
    "                averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "                op_averages = averages.apply([cross_entropy, regularization, loss])\n",
    "                tf.summary.scalar('loss/avg/cross_entropy', averages.average(cross_entropy))\n",
    "                tf.summary.scalar('loss/avg/regularization', averages.average(regularization))\n",
    "                tf.summary.scalar('loss/avg/total', averages.average(loss))\n",
    "                with tf.control_dependencies([op_averages]):\n",
    "                    loss_average = tf.identity(averages.average(loss), name='control')\n",
    "            return loss, loss_average\n",
    "    \n",
    "    def training(self, loss, learning_rate, decay_steps, decay_rate=0.95, momentum=0.9):\n",
    "        \"\"\"Adds to the loss model the Ops required to generate and apply gradients.\"\"\"\n",
    "        with tf.name_scope('training'):\n",
    "            # Learning rate.\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            if decay_rate != 1:\n",
    "                learning_rate = tf.train.exponential_decay(\n",
    "                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            # Optimizer.\n",
    "            if momentum == 0:\n",
    "                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                #optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "            else:\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            op_gradients = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "            # Histograms.\n",
    "            for grad, var in grads:\n",
    "                if grad is None:\n",
    "                    print('warning: {} has no gradient'.format(var.op.name))\n",
    "                else:\n",
    "                    tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "            # The op return the learning rate.\n",
    "            with tf.control_dependencies([op_gradients]):\n",
    "                op_train = tf.identity(learning_rate, name='control')\n",
    "            return op_train\n",
    "\n",
    "    # Helper methods.\n",
    "\n",
    "    def _get_path(self, folder):\n",
    "        path = os.path.dirname(os.path.realpath(__file__))\n",
    "        return os.path.join(path, '..', folder, self.dir_name)\n",
    "\n",
    "    def _get_session(self, sess=None):\n",
    "        \"\"\"Restore parameters if no session given.\"\"\"\n",
    "        if sess is None:\n",
    "            sess = tf.Session(graph=self.graph)\n",
    "            filename = tf.train.latest_checkpoint(self._get_path('checkpoints'))\n",
    "            self.op_saver.restore(sess, filename)\n",
    "        return sess\n",
    "\n",
    "    def _weight_variable(self, shape, regularization=True):\n",
    "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
    "        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        initial = tf.constant_initializer(0.1)\n",
    "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Fully connected\n",
    "\n",
    "\n",
    "class fc1(base_model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def _inference(self, x, dropout):\n",
    "        W = self._weight_variable([NFEATURES, NCLASSES])\n",
    "        b = self._bias_variable([NCLASSES])\n",
    "        y = tf.matmul(x, W) + b\n",
    "        return y\n",
    "\n",
    "class fc2(base_model):\n",
    "    def __init__(self, nhiddens):\n",
    "        super().__init__()\n",
    "        self.nhiddens = nhiddens\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([NFEATURES, self.nhiddens])\n",
    "            b = self._bias_variable([self.nhiddens])\n",
    "            y = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "        with tf.name_scope('fc2'):\n",
    "            W = self._weight_variable([self.nhiddens, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "# Convolutional\n",
    "\n",
    "\n",
    "class cnn2(base_model):\n",
    "    \"\"\"Simple convolutional model.\"\"\"\n",
    "    def __init__(self, K, F):\n",
    "        super().__init__()\n",
    "        self.K = K  # Patch size\n",
    "        self.F = F  # Number of features\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            W = self._weight_variable([self.K, self.K, 1, self.F])\n",
    "            b = self._bias_variable([self.F])\n",
    "#            b = self._bias_variable([1, 28, 28, self.F])\n",
    "            x_2d = tf.reshape(x, [-1,28,28,1])\n",
    "            y_2d = self._conv2d(x_2d, W) + b\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "        with tf.name_scope('fc1'):\n",
    "            y = tf.reshape(y_2d, [-1, NFEATURES*self.F])\n",
    "            W = self._weight_variable([NFEATURES*self.F, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class fcnn2(base_model):\n",
    "    \"\"\"CNN using the FFT.\"\"\"\n",
    "    def __init__(self, F):\n",
    "        super().__init__()\n",
    "        self.F = F  # Number of features\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            # Transform to Fourier domain\n",
    "            x_2d = tf.reshape(x, [-1, 28, 28])\n",
    "            x_2d = tf.complex(x_2d, 0)\n",
    "            xf_2d = tf.fft2d(x_2d)\n",
    "            xf = tf.reshape(xf_2d, [-1, NFEATURES])\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            Wreal = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            Wimg = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            W = tf.complex(Wreal, Wimg)\n",
    "            xf = xf[:int(NFEATURES/2), :, :]\n",
    "            yf = tf.matmul(W, xf)  # for each feature\n",
    "            yf = tf.concat([yf, tf.conj(yf)], axis=0)\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf_2d = tf.reshape(yf, [-1, 28, 28])\n",
    "            # Transform back to spatial domain\n",
    "            y_2d = tf.ifft2d(yf_2d)\n",
    "            y_2d = tf.real(y_2d)\n",
    "            y = tf.reshape(y_2d, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "# Graph convolutional\n",
    "\n",
    "\n",
    "class fgcnn2(base_model):\n",
    "    \"\"\"Graph CNN with full weights, i.e. patch has the same size as input.\"\"\"\n",
    "    def __init__(self, L, F):\n",
    "        super().__init__()\n",
    "        #self.L = L  # Graph Laplacian, NFEATURES x NFEATURES\n",
    "        self.F = F  # Number of filters\n",
    "        _, self.U = graph.fourier(L)\n",
    "    def _inference(self, x, dropout):\n",
    "        # x: NSAMPLES x NFEATURES\n",
    "        with tf.name_scope('gconv1'):\n",
    "            # Transform to Fourier domain\n",
    "            U = tf.constant(self.U, dtype=tf.float32)\n",
    "            xf = tf.matmul(x, U)\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            W = self._weight_variable([NFEATURES, self.F, 1])\n",
    "            yf = tf.matmul(W, xf)  # for each feature\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf = tf.reshape(yf, [-1, NFEATURES])\n",
    "            # Transform back to graph domain\n",
    "            Ut = tf.transpose(U)\n",
    "            y = tf.matmul(yf, Ut)\n",
    "            y = tf.reshape(yf, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class lgcnn2_1(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M, K = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.reshape(x, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, 1, self.F])\n",
    "#            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class lgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.transpose(x)  # M x N\n",
    "            def lanczos(x):\n",
    "                return graph.lanczos(self.L, x, self.K)\n",
    "            xl = tf.py_func(lanczos, [xl], [tf.float32])[0]\n",
    "            xl = tf.transpose(xl)  # N x M x K\n",
    "            xl = tf.reshape(xl, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class cgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xc = tf.transpose(x)  # M x N\n",
    "            def chebyshev(x):\n",
    "                return graph.chebyshev(self.L, x, self.K)\n",
    "            xc = tf.py_func(chebyshev, [xc], [tf.float32])[0]\n",
    "            xc = tf.transpose(xc)  # N x M x K\n",
    "            xc = tf.reshape(xc, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xc, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class cgcnn2_3(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.L = L.toarray()\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = x\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.matmul(x, self.L, b_is_sparse=True)  # N x M\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.matmul(xt1, self.L, b_is_sparse=True) - xt0  # N x M\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class cgcnn2_4(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.transpose(xt)  # N x M\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "class cgcnn2_5(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            xt = tf.expand_dims(xt0, 0)  # 1 x M x N\n",
    "            def concat(xt, x):\n",
    "                x = tf.expand_dims(x, 0)  # 1 x M x N\n",
    "                return tf.concat([xt, x], axis=0)  # K x M x N\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                xt = concat(xt, xt1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                xt = concat(xt, xt2)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            xt = tf.transpose(xt)  # N x M x K\n",
    "            xt = tf.reshape(xt, [-1,self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xt, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "def bspline_basis(K, x, degree=3):\n",
    "    \"\"\"\n",
    "    Return the B-spline basis.\n",
    "\n",
    "    K: number of control points.\n",
    "    x: evaluation points\n",
    "       or number of evenly distributed evaluation points.\n",
    "    degree: degree of the spline. Cubic spline by default.\n",
    "    \"\"\"\n",
    "    if np.isscalar(x):\n",
    "        x = np.linspace(0, 1, x)\n",
    "\n",
    "    # Evenly distributed knot vectors.\n",
    "    kv1 = x.min() * np.ones(degree)\n",
    "    kv2 = np.linspace(x.min(), x.max(), K-degree+1)\n",
    "    kv3 = x.max() * np.ones(degree)\n",
    "    kv = np.concatenate((kv1, kv2, kv3))\n",
    "\n",
    "    # Cox - DeBoor recursive function to compute one spline over x.\n",
    "    def cox_deboor(k, d):\n",
    "        # Test for end conditions, the rectangular degree zero spline.\n",
    "        if (d == 0):\n",
    "            return ((x - kv[k] >= 0) & (x - kv[k + 1] < 0)).astype(int)\n",
    "\n",
    "        denom1 = kv[k + d] - kv[k]\n",
    "        term1 = 0\n",
    "        if denom1 > 0:\n",
    "            term1 = ((x - kv[k]) / denom1) * cox_deboor(k, d - 1)\n",
    "\n",
    "        denom2 = kv[k + d + 1] - kv[k + 1]\n",
    "        term2 = 0\n",
    "        if denom2 > 0:\n",
    "            term2 = ((-(x - kv[k + d + 1]) / denom2) * cox_deboor(k + 1, d - 1))\n",
    "\n",
    "        return term1 + term2\n",
    "\n",
    "    # Compute basis for each point\n",
    "    basis = np.column_stack([cox_deboor(k, degree) for k in range(K)])\n",
    "    basis[-1,-1] = 1\n",
    "    return basis\n",
    "\n",
    "\n",
    "class cgcnn(base_model):\n",
    "    \"\"\"\n",
    "    Graph CNN which uses the Chebyshev approximation.\n",
    "\n",
    "    The following are hyper-parameters of graph convolutional layers.\n",
    "    They are lists, which length is equal to the number of gconv layers.\n",
    "        F: Number of features.\n",
    "        K: List of polynomial orders, i.e. filter sizes or number of hopes.\n",
    "        p: Pooling size.\n",
    "           Should be 1 (no pooling) or a power of 2 (reduction by 2 at each coarser level).\n",
    "           Beware to have coarsened enough.\n",
    "\n",
    "    L: List of Graph Laplacians. Size M x M. One per coarsening level.\n",
    "\n",
    "    The following are hyper-parameters of fully connected layers.\n",
    "    They are lists, which length is equal to the number of fc layers.\n",
    "        M: Number of features per sample, i.e. number of hidden neurons.\n",
    "           The last layer is the softmax, i.e. M[-1] is the number of classes.\n",
    "    \n",
    "    The following are choices of implementation for various blocks.\n",
    "        filter: filtering operation, e.g. chebyshev5, lanczos2 etc.\n",
    "        brelu: bias and relu, e.g. b1relu or b2relu.\n",
    "        pool: pooling, e.g. mpool1.\n",
    "    \n",
    "    Training parameters:\n",
    "        num_epochs:    Number of training epochs.\n",
    "        learning_rate: Initial learning rate.\n",
    "        decay_rate:    Base of exponential decay. No decay with 1.\n",
    "        decay_steps:   Number of steps after which the learning rate decays.\n",
    "        momentum:      Momentum. 0 indicates no momentum.\n",
    "\n",
    "    Regularization parameters:\n",
    "        regularization: L2 regularizations of weights and biases.\n",
    "        dropout:        Dropout (fc layers): probability to keep hidden neurons. No dropout with 1.\n",
    "        batch_size:     Batch size. Must divide evenly into the dataset sizes.\n",
    "        eval_frequency: Number of steps between evaluations.\n",
    "\n",
    "    Directories:\n",
    "        dir_name: Name for directories (summaries and model parameters).\n",
    "    \"\"\"\n",
    "    def __init__(self, L, F, K, p, M, filter='chebyshev5', brelu='b1relu', pool='mpool1',\n",
    "                num_epochs=20, learning_rate=0.1, decay_rate=0.95, decay_steps=None, momentum=0.9,\n",
    "                regularization=0, dropout=0, batch_size=100, eval_frequency=200,\n",
    "                dir_name=''):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Verify the consistency w.r.t. the number of layers.\n",
    "        assert len(L) >= len(F) == len(K) == len(p)\n",
    "        assert np.all(np.array(p) >= 1)\n",
    "        p_log2 = np.where(np.array(p) > 1, np.log2(p), 0)\n",
    "        assert np.all(np.mod(p_log2, 1) == 0)  # Powers of 2.\n",
    "        assert len(L) >= 1 + np.sum(p_log2)  # Enough coarsening levels for pool sizes.\n",
    "        \n",
    "        # Keep the useful Laplacians only. May be zero.\n",
    "        M_0 = L[0].shape[0]\n",
    "        j = 0\n",
    "        self.L = []\n",
    "        for pp in p:\n",
    "            self.L.append(L[j])\n",
    "            j += int(np.log2(pp)) if pp > 1 else 0\n",
    "        L = self.L\n",
    "        \n",
    "        # Print information about NN architecture.\n",
    "        Ngconv = len(p)\n",
    "        Nfc = len(M)\n",
    "        print('NN architecture')\n",
    "        print('  input: M_0 = {}'.format(M_0))\n",
    "        for i in range(Ngconv):\n",
    "            print('  layer {0}: cgconv{0}'.format(i+1))\n",
    "            print('    representation: M_{0} * F_{1} / p_{1} = {2} * {3} / {4} = {5}'.format(\n",
    "                    i, i+1, L[i].shape[0], F[i], p[i], L[i].shape[0]*F[i]//p[i]))\n",
    "            F_last = F[i-1] if i > 0 else 1\n",
    "            print('    weights: F_{0} * F_{1} * K_{1} = {2} * {3} * {4} = {5}'.format(\n",
    "                    i, i+1, F_last, F[i], K[i], F_last*F[i]*K[i]))\n",
    "            if brelu == 'b1relu':\n",
    "                print('    biases: F_{} = {}'.format(i+1, F[i]))\n",
    "            elif brelu == 'b2relu':\n",
    "                print('    biases: M_{0} * F_{0} = {1} * {2} = {3}'.format(\n",
    "                        i+1, L[i].shape[0], F[i], L[i].shape[0]*F[i]))\n",
    "        for i in range(Nfc):\n",
    "            name = 'logits (softmax)' if i == Nfc-1 else 'fc{}'.format(i+1)\n",
    "            print('  layer {}: {}'.format(Ngconv+i+1, name))\n",
    "            print('    representation: M_{} = {}'.format(Ngconv+i+1, M[i]))\n",
    "            M_last = M[i-1] if i > 0 else M_0 if Ngconv == 0 else L[-1].shape[0] * F[-1] // p[-1]\n",
    "            print('    weights: M_{} * M_{} = {} * {} = {}'.format(\n",
    "                    Ngconv+i, Ngconv+i+1, M_last, M[i], M_last*M[i]))\n",
    "            print('    biases: M_{} = {}'.format(Ngconv+i+1, M[i]))\n",
    "        \n",
    "        # Store attributes and bind operations.\n",
    "        self.L, self.F, self.K, self.p, self.M = L, F, K, p, M\n",
    "        self.num_epochs, self.learning_rate = num_epochs, learning_rate\n",
    "        self.decay_rate, self.decay_steps, self.momentum = decay_rate, decay_steps, momentum\n",
    "        self.regularization, self.dropout = regularization, dropout\n",
    "        self.batch_size, self.eval_frequency = batch_size, eval_frequency\n",
    "        self.dir_name = dir_name\n",
    "        self.filter = getattr(self, filter)\n",
    "        self.brelu = getattr(self, brelu)\n",
    "        self.pool = getattr(self, pool)\n",
    "        \n",
    "        # Build the computational graph.\n",
    "        self.build_graph(M_0)\n",
    "        \n",
    "    def filter_in_fourier(self, x, L, Fout, K, U, W):\n",
    "        # TODO: N x F x M would avoid the permutations\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        x = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        # Transform to Fourier domain\n",
    "        x = tf.reshape(x, [M, Fin*N])  # M x Fin*N\n",
    "        x = tf.matmul(U, x)  # M x Fin*N\n",
    "        x = tf.reshape(x, [M, Fin, N])  # M x Fin x N\n",
    "        # Filter\n",
    "        x = tf.matmul(W, x)  # for each feature\n",
    "        x = tf.transpose(x)  # N x Fout x M\n",
    "        x = tf.reshape(x, [N*Fout, M])  # N*Fout x M\n",
    "        # Transform back to graph domain\n",
    "        x = tf.matmul(x, U)  # N*Fout x M\n",
    "        x = tf.reshape(x, [N, Fout, M])  # N x Fout x M\n",
    "        return tf.transpose(x, perm=[0, 2, 1])  # N x M x Fout\n",
    "\n",
    "    def fourier(self, x, L, Fout, K):\n",
    "        assert K == L.shape[0]  # artificial but useful to compute number of parameters\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Fourier basis\n",
    "        _, U = graph.fourier(L)\n",
    "        U = tf.constant(U.T, dtype=tf.float32)\n",
    "        # Weights\n",
    "        W = self._weight_variable([M, Fout, Fin], regularization=False)\n",
    "        return self.filter_in_fourier(x, L, Fout, K, U, W)\n",
    "\n",
    "    def spline(self, x, L, Fout, K):\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Fourier basis\n",
    "        lamb, U = graph.fourier(L)\n",
    "        U = tf.constant(U.T, dtype=tf.float32)  # M x M\n",
    "        # Spline basis\n",
    "        B = bspline_basis(K, lamb, degree=3)  # M x K\n",
    "        #B = bspline_basis(K, len(lamb), degree=3)  # M x K\n",
    "        B = tf.constant(B, dtype=tf.float32)\n",
    "        # Weights\n",
    "        W = self._weight_variable([K, Fout*Fin], regularization=False)\n",
    "        W = tf.matmul(B, W)  # M x Fout*Fin\n",
    "        W = tf.reshape(W, [M, Fout, Fin])\n",
    "        return self.filter_in_fourier(x, L, Fout, K, U, W)\n",
    "\n",
    "    def chebyshev2(self, x, L, Fout, K):\n",
    "        \"\"\"\n",
    "        Filtering with Chebyshev interpolation\n",
    "        Implementation: numpy.\n",
    "        \n",
    "        Data: x of size N x M x F\n",
    "            N: number of signals\n",
    "            M: number of vertices\n",
    "            F: number of features per signal per vertex\n",
    "        \"\"\"\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Rescale Laplacian. Copy to not modify the shared L.\n",
    "        L = scipy.sparse.csr_matrix(L)\n",
    "        L = graph.rescale_L(L, lmax=2)\n",
    "        # Transform to Chebyshev basis\n",
    "        x = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        x = tf.reshape(x, [M, Fin*N])  # M x Fin*N\n",
    "        def chebyshev(x):\n",
    "            return graph.chebyshev(L, x, K)\n",
    "        x = tf.py_func(chebyshev, [x], [tf.float32])[0]  # K x M x Fin*N\n",
    "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature.\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # N*M x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def chebyshev5(self, x, L, Fout, K):\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Rescale Laplacian and store as a TF sparse tensor. Copy to not modify the shared L.\n",
    "        L = scipy.sparse.csr_matrix(L)\n",
    "        L = graph.rescale_L(L, lmax=2)\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        L = tf.sparse_reorder(L)\n",
    "        # Transform to Chebyshev basis\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        x0 = tf.reshape(x0, [M, Fin*N])  # M x Fin*N\n",
    "        x = tf.expand_dims(x0, 0)  # 1 x M x Fin*N\n",
    "        def concat(x, x_):\n",
    "            x_ = tf.expand_dims(x_, 0)  # 1 x M x Fin*N\n",
    "            return tf.concat([x, x_], axis=0)  # K x M x Fin*N\n",
    "        if K > 1:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(L, x0)\n",
    "            x = concat(x, x1)\n",
    "        for k in range(2, K):\n",
    "            x2 = 2 * tf.sparse_tensor_dense_matmul(L, x1) - x0  # M x Fin*N\n",
    "            x = concat(x, x2)\n",
    "            x0, x1 = x1, x2\n",
    "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature pair.\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # N*M x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def b2relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per vertex per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, int(M), int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            #tf.maximum\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def apool1(self, x, p):\n",
    "        \"\"\"Average pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.avg_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "\n",
    "    def _inference(self, x, dropout):\n",
    "        # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        for i in range(len(self.p)):\n",
    "            with tf.variable_scope('conv{}'.format(i+1)):\n",
    "                with tf.name_scope('filter'):\n",
    "                    x = self.filter(x, self.L[i], self.F[i], self.K[i])\n",
    "                with tf.name_scope('bias_relu'):\n",
    "                    x = self.brelu(x)\n",
    "                with tf.name_scope('pooling'):\n",
    "                    x = self.pool(x, self.p[i])\n",
    "        \n",
    "        # Fully connected hidden layers.\n",
    "        N, M, F = x.get_shape()\n",
    "        x = tf.reshape(x, [int(N), int(M*F)])  # N x M\n",
    "        for i,M in enumerate(self.M[:-1]):\n",
    "            with tf.variable_scope('fc{}'.format(i+1)):\n",
    "                x = self.fc(x, M)\n",
    "                x = tf.nn.dropout(x, dropout)\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = self.fc(x, self.M[-1], relu=False)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.54069097676e-17\n",
      "0.0\n",
      "1.29206961125e-16\n",
      "7.21644966006e-16\n",
      "1.27162649987e-16\n",
      "-4.79519245694e-16\n",
      "9.24380866532e-17\n",
      "-1.92178222495e-16\n",
      "-1.86306921233e-16\n",
      "9.60226531363e-17\n",
      "5.50048707845e-17\n",
      "3.28122965133e-17\n",
      "-4.16333634234e-17\n",
      "-2.77555756156e-17\n",
      "-1.92178222495e-16\n",
      "-5.55111512313e-17\n",
      "-1.70099436021e-16\n",
      "0.0\n",
      "0.0\n",
      "2.22044604925e-16\n",
      "0.0\n",
      "-6.22511996106e-16\n",
      "-1.94289029309e-16\n",
      "1.94289029309e-16\n",
      "0.0\n",
      "4.52552767062e-17\n",
      "2.22044604925e-16\n",
      "1.56759094161e-16\n",
      "0.0\n",
      "0.0\n",
      "8.32667268469e-17\n",
      "7.04462421928e-16\n",
      "0.0\n",
      "-5.55111512313e-17\n",
      "0.0\n",
      "-4.80541215746e-17\n",
      "0.0\n",
      "4.49939678294e-17\n",
      "-1.92178222495e-16\n",
      "0.0\n",
      "-1.11022302463e-16\n",
      "-6.42789022715e-17\n",
      "2.22044604925e-16\n",
      "0.0\n",
      "2.22044604925e-16\n",
      "4.49939678294e-17\n",
      "0.0\n",
      "2.22044604925e-16\n",
      "4.49939678294e-17\n",
      "-1.92178222495e-16\n",
      "4.49939678294e-17\n",
      "-1.92178222495e-16\n",
      "0.0\n",
      "0.0\n",
      "-5.55111512313e-17\n",
      "4.49939678294e-17\n",
      "0.0\n",
      "4.49939678294e-17\n",
      "-6.42789022715e-17\n",
      "-1.92178222495e-16\n",
      "2.22044604925e-16\n",
      "4.49939678294e-17\n",
      "2.22044604925e-16\n",
      "2.22044604925e-16\n",
      "0.0\n",
      "-1.92178222495e-16\n",
      "-2.22044604925e-16\n",
      "2.22044604925e-16\n",
      "2.22044604925e-16\n",
      "2.22044604925e-16\n",
      "-5.55111512313e-17\n",
      "1.56759094161e-16\n",
      "-1.92178222495e-16\n",
      "0.0\n",
      "0.0\n",
      "-6.42789022715e-17\n",
      "0.0\n",
      "4.49939678294e-17\n",
      "0.0\n",
      "-6.42789022715e-17\n",
      "2.22044604925e-16\n",
      "4.49939678294e-17\n",
      "-6.42789022715e-17\n",
      "1.11022302463e-16\n",
      "4.49939678294e-17\n",
      "2.22044604925e-16\n",
      "4.49939678294e-17\n",
      "-5.55111512313e-17\n",
      "-1.92178222495e-16\n",
      "0.0\n",
      "0.0\n",
      "-1.92178222495e-16\n",
      "2.22044604925e-16\n",
      "4.49939678294e-17\n",
      "-1.92178222495e-16\n",
      "-6.42789022715e-17\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-1.92178222495e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1259dfd50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import networkx as nx\n",
    "import os\n",
    "import sys\n",
    "sys.path.append( '/Users/cdonnat/Dropbox/Pattern-analysis/')\n",
    "import matplotlib.pyplot as plt\n",
    "from shapes.shapes import *\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import seaborn as sb\n",
    "\n",
    "#sys.path.append('/Users/cdonnat/Desktop/structural_equivalents/')\n",
    "from distances_signature import *\n",
    "from heat_diffusion import *\n",
    "from clustering_via_distances import *\n",
    "import graph_tools \n",
    "#from stats.statistic_checks import *\n",
    "from purity import *\n",
    "from characteristic_functions import *\n",
    "from clustering_comparative_analysis import *\n",
    "from cluster_analysis import *\n",
    "from performance_evaluation import *\n",
    "from pattern_detection.pattern_functions import *\n",
    "#import sys\n",
    "#sys.path.append( '../structural_equivalents/struc2vec_alg/src')\n",
    "#from characteristic_functions import *\n",
    "\n",
    "\n",
    "\n",
    "G=nx.barabasi_albert_graph(100,1)\n",
    "nx.draw(G)\n",
    "\n",
    "A=nx.adjacency_matrix(G).todense()\n",
    "N=A.shape[0]\n",
    "L=rw_laplacian(A)\n",
    "k=2\n",
    "List_patterns=cut_laplacian(A,k)\n",
    "## ok\n",
    "taus=np.arange(0,100,1)\n",
    "chi={i: characteristic_function(List_patterns[i],list(taus)) for i in range(N)}\n",
    "cmap=plt.get_cmap('gnuplot')\n",
    "colors=[cmap(x) for x in np.linspace(0,1,N)]\n",
    "plt.figure()\n",
    "for i in [1,34,56,23,67,99,78,46]:\n",
    "     plt.plot(chi[i][:,1],chi[i][:,2],label=str(i)+ ' with degree '+str(np.sum(A[i,:])),color=colors[i])\n",
    "plt.legend(loc='center left',bbox_to_anchor=(1,0.5))\n",
    "### Can we use this framework to do shape mthing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-205e22638695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m]\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m]\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'p'"
     ]
    }
   ],
   "source": [
    "### try to use the framework\n",
    "common = {}\n",
    "common['dir_name']       = 'test/'\n",
    "common['num_epochs']     = 20\n",
    "common['batch_size']     = 100\n",
    "common['decay_steps']    = 200 / common['batch_size']\n",
    "common['eval_frequency'] = 30 * common['num_epochs']\n",
    "common['brelu']          = 'b1relu'\n",
    "common['pool']           = 'mpool1'\n",
    "C = 3  # number of classes\n",
    "\n",
    "\n",
    "name = 'softmax'\n",
    "params = common.copy()\n",
    "params['dir_name'] += name\n",
    "params['regularization'] = 5e-4\n",
    "params['dropout']        = 1\n",
    "params['learning_rate']  = 0.02\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0.9\n",
    "params['F']              = []\n",
    "params['K']              = []\n",
    "params['p']              = []\n",
    "params['M']              = [C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cnn3(base_model):\n",
    "    \"\"\"Simple convolutional model.\"\"\"\n",
    "    def __init__(self, K, F):\n",
    "        super().__init__()\n",
    "        self.K = K  # Neighborhood size\n",
    "        self.T = T  # input feature_vector size\n",
    "        self.F = F  # Number of features\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            W = self._weight_variable([self.CHANNELS*self.T, self.F])\n",
    "            b = self._bias_variable([self.F])\n",
    "#            b = self._bias_variable([1, 28, 28, self.F])\n",
    "            x_2d = tf.reshape(x, [-1,self.CHANNELS*self.T])\n",
    "            y_2d = tf.matmul(x_2d, W) + b\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "        with tf.name_scope('fc1'):\n",
    "            y = tf.reshape(y_2d, [-1, self.F])\n",
    "            W = self._weight_variable([self.F, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if builg_graph==True:\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # Inputs.\n",
    "            with tf.name_scope('inputs'):\n",
    "                self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n",
    "                self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
    "                self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "\n",
    "            # Model.\n",
    "            op_logits = self.inference(self.ph_data, self.ph_dropout)\n",
    "            self.op_loss, self.op_loss_average = self.loss(op_logits, self.ph_labels, self.regularization)\n",
    "            self.op_train = self.training(self.op_loss, self.learning_rate,\n",
    "                    self.decay_steps, self.decay_rate, self.momentum)\n",
    "            self.op_prediction = self.prediction(op_logits)\n",
    "\n",
    "            # Initialize variables, i.e. weights and biases.\n",
    "            self.op_init = tf.global_variables_initializer()\n",
    "            \n",
    "            # Summaries for TensorBoard and Save for model parameters.\n",
    "            self.op_summary = tf.summary.merge_all()\n",
    "            self.op_saver = tf.train.Saver(max_to_keep=5)\n",
    "        \n",
    "        self.graph.finalize()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
