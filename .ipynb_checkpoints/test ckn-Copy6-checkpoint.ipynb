{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from CKN import *\n",
    "from Nystrom import *\n",
    "from image_processing_utils import *\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "test_batch_size=1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=train_loader.dataset.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.ByteTensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11c86d610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAFRCAYAAAAIKMaWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtcVNe5N/DfDAMIAwix5oIa8K4YNRaPmvpK1YMNxMSI\nlyoIopILXnJQiAKCgiKgNkabqG8gtPYIKpqjVs3R2FhvjTUGtRpFsa/BS4PGBlG5GOW23z/8SAiB\nGdZ27xk28/vmM5+PwfXs/ZCd4WHtWevZOkmSJBAREdkwvbUTICIisjYWQyIisnkshkREZPNYDImI\nyOaxGBIRkc1jMSQiIptnsMRJKktvW+I0RETUTA5u7VQ5bj+vX8uO/fraEQUzESOrGEqShOTkZFy6\ndAkODg5ITU1Fp06dlM6NiIjIImQVwwMHDqCyshK5ubk4e/Ys0tPTsX79eqVzIyIijdHpdKodu7q6\nGgsXLkRRURGqqqoQGRmJ5557Dm+//Ta8vb0BAMHBwQgMDMS2bduwdetW2NvbIzIyEsOHDzd5bFnF\n8NSpUxg2bBgAoH///jh//rycwxARUSuj06m3FGX37t3w8PDAypUrce/ePYwdOxazZ8/GjBkzMG3a\ntLpxxcXFyM7Oxs6dO/HgwQMEBwdj6NChsLe3b/LYsopheXk5XF1dfzyIwYDa2lro9VyPQ0RE6ggM\nDERAQAAAoLa2FgaDAfn5+SgsLMSBAwfg7e2N+Ph4fP311/D19YXBYICLiwu8vb1x6dIlvPDCC00e\nW1YxdHFxQUVFRd2/sxASEREA6KHebVInJycAjyZkUVFRmDt3LiorKzFx4kT4+PggIyMDa9euRe/e\nvX8yYXN2dkZZWZmZvGX45S9/iSNHHq36OXPmDHr06CHnMERE1MrodDrZr+a4efMmwsPDERQUhNGj\nR8Pf3x8+Pj4AAH9/fxQUFMDV1RXl5eV1MRUVFXBzczN5XFnFcNSoUXBwcMDkyZOxfPlyxMfHyzkM\nERFRsxUXFyMiIgLz589HUFAQACAiIgLnzp0DABw/fhx9+vRB3759cerUKVRWVqKsrAyFhYXo3r27\nyWPrLPEIJ+4zJCJqWdTaZ/gfXX8jOzbvm7+Y/PvU1FTs27cPXbp0gSRJ0Ol0mDdvHlauXAl7e3u0\nb98eS5cuhdFoxCeffIKtW7dCkiTMnDkT/v7+Jo/NYkhEZIPUKoaDur0sO/ary/sVzEQMV70QEZHN\ns0g7NiIisg06FVeTqumJZoZnz55FWFiYUrkQEZHG6XV62S9rkj0zzMrKwq5du2A0GpXMh4iIyOJk\nl2IvLy+sW7dOyVyIiEjj1N5nqBbZxXDUqFGws7NTMhciItI4vU4n+2XVvK16diIiohbgiVeTWmCb\nIhERaYROo3OsJy6G1r7PS0RELYdWa8ITlfAOHTogNzdXqVyIiIisgpvuiYhIMdZeCCMXiyERESnG\nJjvQEBERtQayZobV1dVYuHAhioqKUFVVhcjISIwcOVLp3IiISGOs3VZNLlnFcPfu3fDw8MDKlStx\n7949jB07lsWQiIg0u5pUVjEMDAxEQEAAAKC2thYGAz96JCIi7ZJVxZycnAAA5eXliIqKwrx58xRN\nioiItEmrq0ll39y9efMmwsPDERQUhFdeeUXJnIiISKN0T/CPNcmaGRYXFyMiIgKLFy/GkCFDlM6J\niIjIomQVw4yMDJSWlmL9+vVYt24ddDodsrKy4ODgoHR+RESkIVpdTaqTLNBpu7L0ttqnICIiAQ5u\n7VQ57iv9gmXH7v16i4KZiNFmCSciIlIQ90QQEZFitLqalMWQiIgUY+1VoXLJKoa1tbVITEzElStX\noNfrsWTJEnTr1k3p3IiIiCxC1meGBw8ehE6nw5YtWxAVFYX3339f6byIiEiDdDqd7Jc1yZoZ+vv7\n1/UiLSoqQtu2bRVNioiItMnmPjPU6/WIi4vDgQMH8MEHHyiZExERkUU90QKa5cuX4/bt25g4cSL2\n7t2LNm3aKJUXERFpkE0toNm1axdu3bqFt956C46OjtDr9dDruWWRiMjWabUDjaxi+Jvf/Abx8fEI\nDQ1FdXU1EhIS2IqNiIg0S/YjnNasWaN0LkREpHHWXhUqFzfdExGRYrS6mlSbN3eJiIgUxJkhEREp\nRqurSZ9oZnj79m0MHz4cV65cUSofIiLSML1OJ/tl1bzlBlZXVyMpKYl7C4mISPNkF8MVK1YgODgY\nTz/9tJL5EBGRhmm1N6msYrhjxw60a9cOQ4cOhSRJSudEREQaZVO3SXfs2IFjx44hLCwMBQUFiI2N\nxe3bt5XOjYiIyCJkrSbNycmp+3NYWBiWLl2Kdu3aKZYUERFpk1ZXkz7x1gpr3+clIqKWw9q3O+V6\n4mK4ceNGJfIgIiKyGm66JyIixWj1biGLIRERKcZmb5MStQRSTbVwTOW9uypk8uS+/L8HhWPu/yD+\n/V+6XiI0/u3lQcLn2Ji0Rzjm4y//IhzTxl6s+ceycWOFzzE88bfCMaQdsovhuHHj4OLiAgDo2LEj\n0tLSFEuKiIi0yaZWk1ZWVgLg4hkiIvoprd4mlbXpvqCgAPfv30dERASmTZuGs2fPKp0XERGRxcia\nGbZp0wYRERGYOHEirl69ijfffBP79++HXs/HIxIR2TKbWk3q7e0NLy+vuj+7u7vj+++/xzPPPKNo\nckREpC02dZt0+/btWL58OQDg1q1bqKioQPv27RVNjIiIyFJkzQwnTJiA+Ph4hISEQK/XIy0tjbdI\niYhI1duk1dXVWLhwIYqKilBVVYXIyEh069YNcXFx0Ov16N69O5KSkgAA27Ztw9atW2Fvb4/IyEgM\nHz7c5LFlFUN7e3u89957ckKJiKgVU3Nrxe7du+Hh4YGVK1eitLQUr7/+Onr16oXo6GgMHDgQSUlJ\nOHDgAF588UVkZ2dj586dePDgAYKDgzF06FDY29s3eWxuuiciIk0IDAxEQEAAAKCmpgZ2dna4cOEC\nBg4cCADw8/PDsWPHoNfr4evrC4PBABcXF3h7e+PSpUt44YUXmjw2720SEZFi9Dr5L3OcnJzg7OyM\n8vJyREVFYd68eT95wLzRaER5eTkqKirg6upa93VnZ2eUlZWZPDZnhjas4l/XhGNqH1YJjf/275eF\nz/GPkzeEY+6UPxSO+e9T4m3PWhNv9w5C4w0Ju4TPsfH0IeEYtzau5gc1MLhjb6HxPYd2Ej4HNY/a\nWytu3ryJOXPmIDQ0FKNHj8bvfve7ur+rqKiAm5sbXFxcUF5e/rOvmyJ7ZpiZmYnJkydj/Pjx2L59\nu9zDEBERNUtxcTEiIiIwf/58BAU96pXbu3dv5OXlAQCOHj0KX19f9O3bF6dOnUJlZSXKyspQWFiI\n7t27mzy2rJnhV199hX/84x/Izc3F/fv38cc//lHOYYiIqJVRc59hRkYGSktLsX79eqxbtw46nQ4J\nCQlYtmwZqqqq0LVrVwQEBECn0yEsLAwhISGQJAnR0dFwcHAweWxZxfCLL75Ajx49MGvWLFRUVGDB\nggWyvjEiImpd1LxNmpCQgISEhJ99PTs7+2dfmzhxIiZOnNjsY8sqhnfu3MGNGzeQkZGBf/3rX5g5\ncyY+++wzOYciIiKyOlnF0N3dHV27doXBYEDnzp3h6OiIkpISPPXUU0rnR0REGqLX6COcZC2g8fX1\nxd/+9jcAj9qxPXjwAB4eHoomRkRE2qPT6WS/rEnWzHD48OE4efIkJkyYAEmSkJSUZPVvhIiISC7Z\n+wzfffddJfMgIqJWQKtPreCmeyIiUoxGayHbsREREXFm2EqUnD4rHBM880PhmDsP7gnHkPrkrOBb\n+kaA0HhHo+lNy40Z/Vov4RgXz7bCMY4eLkLj3Xr2FD4HNY9N3SbduXMnduzYAZ1Oh4cPH6KgoADH\njh2Di4vY/5BERNS6qPkIJzXJKoZBQUF1feGWLl2KCRMmsBASEZFmPdFnhufOncPly5eFWt4QEVHr\nZVP7DB/LzMzEnDlzlMqFiIg0zqY+MwSAsrIyXL16FYMGDVIyHyIi0jCN1kL5t0nz8vIwZMgQJXMh\nIiKyCtkzwytXrqBTJz4tmoiIfmRzt0kjIiKUzIOIiMhquOmeiIgUY1P7DImIiBpjc7dJqWUxencQ\njnnaKP4wZltvxza88wCh8e2cnYXPsafgK+EYRztH4ZheYWLt2IiaQ6O1UF4xrK6uRmxsLIqKimAw\nGJCSkoLOnTsrnRsREZFFyNpaceTIEdTW1iI3NxezZs3C6tWrlc6LiIg0SKsdaGQVQ29vb9TU1ECS\nJJSVlcHe3l7pvIiIiCxG1m1So9GIb7/9FgEBAbh79y4yMjKUzouIiDRIqwtoZM0M//SnP2HYsGHY\nv38/du/ejdjYWFRWViqdGxERaYxOJ/9lTbJmhm3btoXB8CjU1dUV1dXVqK2tVTQxIiLSHq3ODGUV\nw/DwcCxcuBBTpkxBdXU1YmJi0KZNG6VzIyIisghZxdDZ2Rlr1qxROhciItI4rXageaKH+xIREbUG\n7EBDRESKsfZ+QblYDFsJx6d+IRyz9B3xdlzH/npFaHwfn/bC53g7I1s4Ro4hHV8Qjknb9F9C4w1G\nF+FzvJ1/QThmb8aXwjFEatBrsxbKK4aVlZWIj4/Ht99+CxcXFyQlJeH5559XOjciItIYrc4MZX1m\n+Mknn8BoNGLr1q1ITEzEkiVLlM6LiIjIYmTNDC9fvgw/Pz8AQOfOnVFYWKhoUkREpE02NTPs3bs3\nDh8+DAA4c+YM/v3vf0OSJCXzIiIiDdLr5L+smrecoPHjx8NoNGLKlCn461//ij59+mj2twEiIiJZ\nt0nPnTuHl156CfHx8Th//jxu3LihdF5ERKRBWp0YySqGXl5e+P3vf4+PPvoIbm5uSE1NVTovIiLS\nII3WQnnF0MPDAxs2bFA6FyIiIqvgpnsiIlKMTT21goiIqDFabdTNYmjDOgeNFI7p+J93hcbbu7YV\nPsfcgmLhmDVH9gnHzAkZIhwjp72aKPc+PsIxIR+IxxDRj5q1teLs2bMICwsDAFy/fh0hISEIDQ1l\n5xkiIvoJrT7p3mwxzMrKQmJiIqqqqgAA6enpiI6ORk5ODmpra3HgwAHVkyQiIm3Q63SyX1bN29wA\nLy8vrFu3ru7f8/PzMXDgQACAn58fjh8/rl52REREFmC2GI4aNQp2dnZ1/16/7ZrRaERZWZk6mRER\nkebodDrZL2sSXkCj1/9YPysqKuDm5qZoQkREpF3W/uxPLuHepD4+PsjLywMAHD16FL6+voonRURE\nZEnCM8PY2FgsWrQIVVVV6Nq1KwICxJ+WTkRErZO1b3fK1axi2KFDB+Tm5gIAvL29kZ2drWpSRESk\nTdZ+FJNcsh7hRERE1JqwAw0RESmmVd8mJXrM3s1d9XO0dXFU/RwAsGnn18IxycH+QuN1ejvzg4ha\nEY3WQvF2bI+lp6dj69atqiRFRETUmPr16OLFi/Dz88PUqVMxdepU7Nv3qEfxtm3bMH78eEyePBmH\nDx9u1nHNzgyzsrKwa9cuGI1GAEBJSQliY2Nx7do1dOnSRea3Q0RErZGabdUa1qPz589jxowZmDZt\nWt2Y4uJiZGdnY+fOnXjw4AGCg4MxdOhQ2Nvbm87b3MkbtmO7f/8+3nnnHYwZM0bmt0NERK2Vmh1o\nGmsPevjwYYSGhiIxMREVFRX4+uuv4evrC4PBABcXF3h7e+PSpUtmjy3cjq1jx47o16+f2QMTEREp\nqWE96t+/PxYsWICcnBx06tQJa9euRXl5OVxdXevGODs7N6ttKLdWEBGRYiz5CCd/f3/4+PjU/bmg\noACurq4oLy+vG9PctqHNLob1G3QTERE1xpKNuiMiInDu3DkAwPHjx9GnTx/07dsXp06dQmVlJcrK\nylBYWIju3bubPVazt1Zode8IERG1TsnJyUhJSYG9vT3at2+PpUuXwmg0IiwsDCEhIZAkCdHR0XBw\ncDB7LJ1kgSlfZelttU9Brcj/LtwkHLPkf/8sHBPYfZBwTPK2aKHx3GdILZWDWztVjrv5zfdlx4Z8\nLPb+UhI33RMRkWKs/cR6ubiAhoiIbB5nhtTivJzwunDMV/+8IRyz7/99JRzz5ufHhcZ3fPn/CJ+D\nSMs0OjEUb8d28eJFTJkyBVOnTsUbb7yBkpISVRMkIiLtsORqUiWZLYZZWVlITExEVVUVACAtLQ2L\nFy/Gxo0bMWrUKGRmZqqeJBERkZqE27GtXr0aPXv2BABUV1fD0dEyTxggIqKWz5Kb7pUk3I7tF7/4\nBQDg9OnT2Lx5808apBIRkW3T6m1SWQto9u7di4yMDGRmZsLDw0PpnIiIiCxKuBju2rUL27ZtQ3Z2\ndrP6vRERke2w9u1OuYSKYW1tLdLS0uDp6YnZs2dDp9Nh0KBBmDNnjlr5ERGRhmh1032zimGHDh2Q\nm5sLADhx4oSqCREREVkaN90TEZFiNDoxZDEkIiLlWHtVqFwshtTiGIwuwjHzV44Tjvli0gXhmOi0\n/xEa/8qfC4TP0e+Fp4VjfjnrNeEYzf4KT6QC4XZsly9fRkhICEJCQhAfH4/a2lpVEyQiIu1otZvu\nG7ZjW716NWJiYrB582YAwMGDB9XNkIiINEOrm+6F27GtXbsWvr6+qKysxPfffw9XV1dVEyQiIlKb\ncDs2nU6HGzdu4LXXXsPdu3fRq1cvVRMkIiLtaLW3SRvj6emJ/fv3Y9KkSUhPT1c6JyIi0qhWe5u0\noZkzZ+LatWsAAKPRCL1eVj0lIiJqMYS3Vrz11luIi4uDg4MDnJycsGzZMjXyIiIiDbL27U65hNux\nDRgwAFu2bFE1KSIi0iZr3+6Ui/c4iYjI5rEDDRERKUajE0MWQ2odXLp0FY7JWhgmHPNmeo7Q+PV/\n3y98DvxdPGTN/SrhmAFhLwmNd3rWU/gcZHu0+ggn4XZsj+3ZsweTJ09WJSkiItImre4zNDszzMrK\nwq5du2A0Guu+duHCBWzfvl3VxIiIiCxFuB3bnTt3sGbNGiQkJKiaGBERaU+r3XRfvx1bbW0tEhMT\nERcXBycnJ0iSpHqCREREahPaWpGfn4/r168jOTkZMTEx+Oabb9iOjYiI6rTazwwfkyQJffv2xZ49\newAARUVFiImJQXx8vGrJERGRtuj02lxN2uxiaO37uURE1PJptVQ06zZp/XZspr5GRESkRdx0T0RE\nitHqXUT2JiUiIpvHmSHZrM5BI4Vj/qfbM0LjP1z2mfA59hR8KRwzN0f8I4t3b5QJjX/1XX/hczh3\n6CgcQ9qm0YmheDu2ixcvws/PD1OnTsXUqVOxb98+VRMkIiLt0Oqme+F2bOfPn8eMGTMwbdo0tXMj\nIiKNabUzw4bt2PLz83H48GGEhoYiISEB9+/fVzVBIiIitQm1YwOA/v37Y8GCBcjJyUGnTp3w4Ycf\nqpogERFpiEZb0AivJvX394ePjw+AR4WyoKBA8aSIiIgsSbgYRkRE4Ny5cwCA48ePo0+fPoonRURE\n2tRqF9A0lJycjJSUFNjb26N9+/ZYunSpGnkREZEGaXUBTbOKYf3Waz4+PtiyZYuqSRERkTZptVE3\nO9AQEZHNYwcaIiJSTKu+TUpEj7j3FVswFpvhKXyOiXvEF6VNfe8PwjHvHfxfofGXbtwWPseirTHC\nMUTWINyOraSkBLNmzUJYWBhCQkLwr3/9S9UEiYhIO1rtatKG7dh+97vfYcyYMQgICMCJEydQWFiI\nTp06qZ4oERG1fFq9TSrcju306dP47rvvMH36dHz66acYPHiwqgkSEZF2aHVmKNyOraioCO7u7tiw\nYQOeffZZZGZmqpogERGR2oS3Vri7u2PEiBEAgJEjRyI/P1/xpIiISJs02ppUvBj6+vriyJEjAIC8\nvDx069ZN8aSIiIgaU39B5/Xr1xESEoLQ0FAsWbKkbsy2bdswfvx4TJ48GYcPH27WcYWLYWxsLP78\n5z8jODgYX3zxBSIjI0UPQURErZSanxlmZWUhMTERVVVVAID09HRER0cjJycHtbW1OHDgAIqLi5Gd\nnY2tW7ciKysLq1atqhtvinA7Nk9PT/zxj39sThgREdkaFfuaPV7QuWDBAgCPnq87cOBAAICfnx+O\nHTsGvV4PX19fGAwGuLi4wNvbG5cuXcILL7xgrbSJiMjWqDkzbLigU5Kkuj8bjUaUl5ejoqICrq6u\ndV93dnZGWVmZ2WOzGBIRkSbp9T+WsIqKCri5ucHFxQXl5eU/+7o5bMdGpCIHdw/hmF5hAcIx9u//\nt3BMVW210PjP/nlS+BxvHDouHPPciJeEY6jlsOSqUB8fH+Tl5eE//uM/cPToUQwZMgR9+/bF6tWr\nUVlZiYcPH6KwsBDdu3c3e6xmFcOzZ8/ivffeQ3Z2NqKjo1FcXAxJklBUVIQBAwZg1apVT/xNERER\niYiNjcWiRYtQVVWFrl27IiAgADqdrq5dqCRJiI6OhoODg9lj6aT6N10bUb8d2+NFNABQWlqK8PBw\nZGVloV27diZPUlkq3uCXiJpv6IBg4RjRmaG9XvxG0v+8/1/CMZwZWoaDm+mf23L9Y0227NgBc8MU\nzESMcDu2xz744AOEhoaaLYRERGQ7Wu2m+4ard4BHT644ceIExo0bp1piRESkQRqthrJWk3722Wd4\n9dVXrd5YlYiISAnNLob1P1o8fvw4/Pz8VEmIiIi0S6fXyX5ZU7OLYf1Z4NWrV/kMQyIiajWE27EB\nwJ49e1RLiIiItEurn55x0z0RESlGq2tJWAyJiEgxGq2F4h1oLl68iOTkZBgMBnh7eyM1NVXtHIla\njDtfnxcaf2qn2HgAOFnwnXCM6AZ6OV58todwzLO/HqRCJkTKM7uApuHzo9atW4c5c+Zg06ZNePjw\nYbMfnEhERDagte4zbNiBpnfv3rhz5w4kSUJFRQUMBt5pJSIibRPuQPP41ujo0aNRUlKCQYN4G4SI\niB5p9fsMH0tNTcXmzZuxd+9ejBkzBsuXL1cjLyIi0iCN3iUVL4bu7u5wcXEBADzzzDMoLS1VPCki\nItIojVZD4Q/8UlJSMHfuXBgMBjg4OCAlJUWNvIiIiCxGuAONr68vtmzZompSRESkTda+3SmXrKdW\nEBERtSbcF0FERIqx9qpQuVgMiYhIMa26N2n9dmz5+flITk6Go6MjevXqhcTERLVzJDKr9NIl4Zi/\nZH4pHLP5q5NC478tFW+tZil2OrFPSZ738BA+h05vZ34QtS7arIXi7dgWL16MxMRE5OTkwNXVlY9z\nIiIizRNux3br1i30798fADBgwACcOnVKveyIiEhTdDqd7Jc1Cbdj69SpE06efHSr6NChQ/jhhx/U\ny46IiMgChBfQpKWlITU1FTU1NfD19YWjo6MaeRERkQZZe4Ynl/A+wyNHjmDVqlXYsGED7t69i1/9\n6ldq5EVERFqkf4KXFQnPDL28vBAeHg4nJycMHjwYfn5+auRFREQapNWZoXA7thEjRmDEiBGqJkVE\nRGRJ3HRPRESK0erMkL1JiYjI5nFmSEREytHmxNB0MayursbChQtRVFSEqqoqREZGolu3boiLi4Ne\nr0f37t2RlJRkqVxJg3747oZwzNdbTgjH/H73MeGYyyXXhGNaqv/zfD/hmKg3hgmN93p9uPA5yPa0\nykbdu3fvhoeHB1auXInS0lK8/vrr6NWrF6KjozFw4EAkJSXhwIED8Pf3t1S+RETUkrXGzwwDAwMR\nFRUFAKipqYGdnR0uXLiAgQMHAgD8/Pxw/Phx9bMkIiJSkcli6OTkBGdnZ5SXlyMqKgrz5s2DJEl1\nf280GlFWVqZ6kkREpA06nfyXNZldTXrz5k2Eh4cjKCgIo0ePhl7/Y0hFRQXc3NxUTZCIiEhtJoth\ncXExIiIiMH/+fAQFBQEAevfujby8PADA0aNH4evrq36WRESkCVp9aoXJBTQZGRkoLS3F+vXrsW7d\nOuh0OiQkJGDZsmWoqqpC165dERAQYKlciYiopWuNq0kTEhKQkJDws69nZ2erlhAREWmXtWd4crED\nDRER2Tx2oCEiIuVoc2LImSEREZFwO7aRI0cCANLT09GlSxdMmjTJIomS8h7cuikccyf/qtD4xcs/\nFT7H2Vv/FI5pqYZ3HiAcMzN8iHCM9+u/Fo7R6e2EY4jM0epnhs1ux3bv3j2MHTsWAwYMwIIFC3Dt\n2jV06dLFUnkSEZEGtMrepIGBgXVbJ2pra2EwGHD//n288847OHr0qEUSJCIiDdHozFC4HVuHDh3Q\nr594h3wiImr9tLrpXqgd2yuvvGKJnIiIiCzK5G3Sx+3YFi9ejCFDxD/UJyIi0gKTM8P67djCwsIw\ndepUVFZWWio3IiLSGt0TvKxIVjs2AJgzZ44qCRERkXa1ytWkREREQjS6mpTFkIiIFGPtVaFysR0b\nERHZPOF2bJ6enkhJSYGdnR0cHBywcuVKPPXUU5bK1yY8LCkWjvk4ZptwzIlrV4VjLt2+IhzTUv1n\nF/EHU781dZDQ+E4Bg4XPYXAyCscQ2Ypx48bBxcUFANCxY0dERkYiLi4Oer0e3bt3R1JSkqzjNrsd\nW2lpKV5//XV07NgRixcvRs+ePbF161ZkZmYiLi5O1smJiKiVUXEBzePdDBs3bqz72syZMxEdHY2B\nAwciKSkJBw4cgL+/v/Cxm92OraamBgaDAWvWrEG7du0APJo5Ojo6Cp+UiIhaJzU/MywoKMD9+/cR\nERGBmpoazJs3DxcuXMDAgQMBAH5+fvj73/+ufDF0cnICgJ+0Y3tcCE+fPo3NmzcjJydH+KRERNRK\nqbh+pk2bNoiIiMDEiRNx9epVvPnmm5Akqe7vjUYjysrKZB3b7GrSmzdvYs6cOQgNDa1rx7Z3715k\nZGQgMzMTHh4esk5MREStj5ozQ29vb3h5edX92d3dHRcuXKj7+4qKCri5uck6tsnVpI/bsc2fPx9B\nQUEAgF27dmHTpk3Izs5Ghw4dZJ2UiIhI1Pbt27F8+XIAwK1bt1BeXo6hQ4fiq6++AgAcPXoUvr7i\nC+MAMzPD+u3Y1q1bh9raWly+fBmenp6YPXs2dDodBg0axG40RESkugkTJiA+Ph4hISHQ6/VYvnw5\n3N3dkZiOXER8AAARXUlEQVSYiKqqKnTt2rVunYso2e3YiIiIfkbF1aT29vZ47733fvb17OzsJz42\nO9AQEZFitNqBhsWQiIiUw2JoG4pPnBaOyf3oS6Hxh7+5JHyO6/duCMe0VM72TsIxC0cHCseMXPCa\ncIzB6CIcQ2RLWuXMsLF2bF5eXli0aBEAwMvLC6mpqdDr2eKUiIi0S7gdW58+fRATEwNfX1/Ex8fj\n4MGDsnb7ExERtRTC7djWrl0L4FGPuO+//x6urq7qZ0lERNrQGh/u21g7NgC4ceMGpk+fDldXV/Tq\n1Uv9LImISBO0+pmh2Q/7bt68ifDwcAQFBdW1Y/P09MT+/fsxadIkpKenq54kERFphE4n/2VFwu3Y\nZs6ciWvXrgF41BSVi2eIiOgxnV4n+2VNQu3YdDod5s2bh7i4ODg4OMDJyQnLli2zVK5ERESqkNWO\nbcuWLaolREREZGncdE9ERMrR6AIaFkMiIlKMVleTshgKyttTIByz8fQhFTJ5cv2f6SEcM35wH6Hx\ndnbiC6x+Pfdl4RgHdz5kmqhF0GgxNPmTqrq6GgsWLMCUKVPw29/+FgcPHqz7uz179mDy5MmqJ0hE\nRNrRKleT1m/Hdu/ePYwdOxYjR47EhQsXsH37dkvlSEREpCqTM8PAwEBERUUBAGpra2EwGHD37l2s\nWbOGD/0lIqJWQ6gdW1RUFBISEur2GUqSZJEkiYhII1rjZ4bAT9uxPf/887h+/TqSk5MRExODb775\nhu3YiIjoRxptx2ZyZvi4HdvixYsxZMgQAI8WzgBAUVERYmJiEB8fr36WRESkCVrdWmFyZli/HVtY\nWBimTp2KyspKS+VGRERao9fJf1mRrHZsANChQwfk5uaqkhQREZEl8ZETRERk89iBhoiIFKPTaXOO\npZMssD+isvS22qcgIiIBDm7tVDnu3YtnZce69+6vYCZiTM4Mq6ursXDhQhQVFaGqqgqRkZF47rnn\n8Pbbb8Pb2xsAEBwcjMDAQEvkSkRELZxWV5MKt2ObPXs2ZsyYgWnTplkoRSIi0gwrrwqVy2QxDAwM\nREBAAIAf27Hl5+ejsLAQBw4cgJeXFxISEuDs7GyRZImIiNTQrM8My8vLMWvWLEyaNAmVlZXo2bMn\nfHx88NFHH+HevXuIjY01Gc/PDImIWha1PjO8989zsmPb9uirYCZihNqxjR49Gv7+/vDx8QEAjBo1\nCgUF4s/3IyKi1kmn08l+WZPJYvi4Hdv8+fMRFBQEAIiIiMC5c48q//Hjx9Gnj9jDXomIqBXTaG9S\nk7dJU1NTsW/fPnTp0gWSJEGn02HevHlYuXIl7O3t0b59eyxduhRGo9HkSXiblIioZVHrNmnpNxdl\nx7p17a1gJmK4z5CIyAapVQzLrsj/6My1cy8FMxGjzVYBRERECmIxJCIim8fepEREpJzW2IGmsXZs\nL774IhITE1FWVoaamhqsWLECnTp1slS+RETUgll7i4Rcwu3YhgwZgjFjxiAgIAAnTpxAYWEhiyER\nET2i0adWCLVjs7Ozw+nTp9GzZ09Mnz4dHTt2bPLhv0REZHt0Gu1NarKEOzk5wdnZGeXl5YiKisK8\nefNQVFQEd3d3bNiwAc8++ywyMzMtlSsREZEqhNuxubu7Y8SIEQCAkSNHIj8/X/UkiYiI1CTcjs3X\n1xdHjhwBAOTl5aFbt27qZ0lERNpgK+3YVqxYgYSEBPzwww9wdXXFqlWr4OrqavIk7EBDRNSyqNWB\n5v6NK7JjnT07K5iJGLZjIyKyQaoVw5vXZMc6P+elYCZiuOmeiIiU0xpXkxIREdkCFkMiIrJ5wu3Y\nPv30UxQXF0OSJBQVFWHAgAFYtWqVpfIlIqIWzGbasR06dAgAUFpaivDwcCxcuNAiiRIRkQbYQjs2\ng+HH4R988AFCQ0PRrp06K5KIiEh7tDozFG7HBgAlJSU4ceIExo0bZ5EkiYhII3R6+S8rEmrH9sor\nrwAAPvvsM7z66qua/Q2AiIioPuF2bABw/Phx+Pn5qZ4cERGRJZj8zDAjIwOlpaVYv3491q1bB51O\nh48//hhXr17lMwyJiOhn1HyEkyRJSE5OxqVLl+Dg4IDU1FTFahHbsRER2SC12rE9vPtv2bGO7k+b\n/PvPP/8cBw8eRHp6Os6ePYuMjAysX79e9vnqYzs2IiJSjE7FhTCnTp3CsGHDAAD9+/fH+fPnFTs2\niyERESlHxYWV5eXlP3lKksFgQG1tLfT6Jy/AFimGak3HiYioZVHz572LiwsqKirq/l2pQgiwNykR\nEWnEL3/5y7qHy585cwY9evRQ7NgWWUBDRET0pOqvJgWA9PR0dO6szAOBWQyJiMjm8TYpERHZPBZD\nIiKyeSyGRERk86yyz/BJWuqcPXsW7733HrKzs82ObezhxCNHjjQZU1tbi8TERFy5cgV6vR5LlixB\nt27dzJ7r9u3bGD9+PDZs2NCsD3THjRsHFxcXAEDHjh2RlpZmNiYzMxMHDx5EVVUVQkJCMH78eJPj\nd+7ciR07dkCn0+Hhw4coKCjAsWPH6s7bUHV1NWJjY1FUVASDwYCUlBSz30tlZSXi4+Px7bffwsXF\nBUlJSXj++eebHF//+l2/fh1xcXHQ6/Xo3r07kpKSzMY8lp6eji5dumDSpEkmx1+8eBHLli2DnZ0d\nHBwcsHLlSjz11FMmYy5fvozFixcDALy8vJCamtro8u3G8tqzZw82bdqE3Nxcs9/LxYsX8fbbb8Pb\n2xsAEBwcjMDAwCbHl5SUIDExEWVlZaipqcGKFSsafd/Uj4mOjm7Ww7gb5pWcnAyDwQBvb2+kpqaa\n/V7y8/ORnJwMR0dH9OrVC4mJiT8Z29h7sVu3bk1ef1Pv3aaufWMxnp6eSElJafL6Nxbj5eWFRYsW\nAWj8+pvKrbHr39j45557zuS1byzmxRdfNHn9+TD2JyRZwV/+8hcpLi5OkiRJOnPmjDRz5sxmxX38\n8cfSq6++Kk2aNKlZ47dv3y6lpaVJkiRJd+/elYYPH2425vPPP5cWLlwoSZIknThxolm5VVVVSbNn\nz5ZefvllqbCw0Oz4hw8fSkFBQWbH1XfixAkpMjJSkiRJqqiokD788EOh+CVLlkjbtm0zOebAgQPS\n3LlzJUmSpGPHjknvvPOO2ePm5ORIixYtkiRJkgoLC6UZM2Y0Obbh9YuMjJTy8vIkSZKkxYsXS59/\n/rnZmNu3b0tvvPGGNGrUKCk3N9fs+NDQUKmgoECSJEnKzc2V0tPTzcbMmjVLOnnypCRJkhQXF9es\nvCRJkvLz86Xw8PAm//9sGLNt2zZpw4YNjY5tbHxcXJy0b98+SZIk6csvv5QOHz7crLwkSZLu3bsn\njR07ViouLjYbM3v2bOno0aOSJElSTEyMdOjQIbMx48aNk86cOSNJkiStWbNG2r1790/G138v3rt3\nTxo+fLjJ69/Ye7ekpMTktW/sHOauf2Mxs2fPNnn9m/q50tT1b2z8J598YvLaNxZj7vqb+nln6vrT\nI1a5TSq3pY6XlxfWrVvX7PMEBgYiKioKwM8fTtwUf39/pKSkAACKiorQtm1bszErVqxAcHAwnn7a\ndF+9xwoKCnD//n1ERERg2rRpOHv2rNmYL774Aj169MCsWbMwc+ZMjBgxolnnAoBz587h8uXLmDhx\noslx3t7eqKmpgSRJKCsrg729vdljX758ue4JJp07d0ZhYWGTYxtev/z8fAwcOBAA4Ofnh+PHj5uN\nuX//Pt555x2MGTOmWedYvXo1evbsCeDRb86Ojo5mY9auXQtfX19UVlbi+++//0nHi6Zi7ty5gzVr\n1iAhIUHo+z98+DBCQ0ORkJCA+/fvmxx/+vRpfPfdd5g+fTo+/fRTDB482Ow5HjP1MO6GMb1798ad\nO3cgSRIqKioafd80jLl16xb69+8PABgwYABOnTr1k/H134s1NTWws7PDhQsXmrz+jb13zV37hucw\nGAxYs2aNyevfWIy5699Ybnfv3m3y+jc2Pj8/H4cOHWry2jeMsbOzM3v9Tf2848PYzbNKMWyqpY45\no0aNgp2dXbPP09TDic3R6/WIi4tDamoqXnvtNZNjd+zYgXbt2mHo0KGQmrlLpU2bNoiIiMAf/vAH\nJCcn49133zX7/d+5cwfnz5/HBx98gOTkZMTExDTrXMCj26tz5swxO85oNOLbb79FQEAAFi9ejLCw\nMLMxvXv3xuHDhwE82gT773//u8n/Dg2vX/1xRqMRZWVlZmM6duyIfv36NZlPw/G/+MUvADwqJJs3\nb8a0adPMxuh0Oty4cQOvvfYa7t69i169epmMeXxrPS4uDk5OTs3+/vv3748FCxYgJycHnTp1wocf\nfmhyfFFREdzd3bFhwwY8++yzyMzMNHsOwPzDuBvGPL41Onr0aJSUlGDQoEFmYzp16oSTJ08CAA4d\nOoQffvjhJ+Mbey+auv6Nje/QoYPJa99YzOMf/k1d/6Z+Rpi6/g1joqKikJCQ0OT1bzh+7ty56Nev\nH2JjY5u89o3lZe7682HsT8YqxVDNljoNNfZw4uZYvnw59u/fj8TERDx48KDJcTt27MCxY8cQFhaG\ngoICxMbG4vZt00/p8Pb2rvvt1tvbG+7u7vj+++9Nxri7u2PYsGEwGAzo3LkzHB0dUVJSYvb7KCsr\nw9WrVxv9gdbQn/70JwwbNgz79+/H7t27ERsbi8rKSpMx48ePh9FoxJQpU/DXv/4Vffr0afZDn+tf\n84qKCri5uTUrTtTevXuxZMkSZGZmwsPDo1kxnp6e2L9/PyZNmoT09HSTY/Pz83H9+vW6X1K++eYb\nszHAo7sQPj4+AB4Vl4KCApPj3d3d6+4IjBw5Evn5+c36XkQfxp2amorNmzdj7969GDNmDJYvX242\nJi0tDR999BGmT5+Odu3aNfrfuf57cfTo0Wavv5z3bmMx5q5/YzHmrn/9mOeff97s9W/4vTfn2jeM\nac7158PY5bNKMXzSljrNnYE19XBiU3bt2lX3G5ejoyP0er3JQp2Tk4Ps7GxkZ2ejV69eWLFihdlb\nEdu3b6/7AXPr1i1UVFSgffv2JmN8fX3xt7/9rS7mwYMHzfrBnpeXhyFDhpgdBwBt27atW1zj6uqK\n6upqszPWc+fO4aWXXsKmTZvw8ssvCz1bzMfHB3l5eQCAo0ePwtfXt8mxzb3mDe3atQubNm1CdnY2\nOnTo0KyYmTNn4tq1awAezVhMXX9JktC3b1/s2bMHGzduxPvvv49u3bohPj7e7HkiIiJw7tw5AI8e\nmN2nTx+T4319feveN3l5eSYXdtX/7yX6MG53d/e6/w+eeeYZlJaWmo05cuQIVq1ahQ0bNuDu3bv4\n1a9+9ZO/b+y92Lt37yavv5z3bmMx5q5/YzHmrn/DmH79+pm8/o2dw9y1byzG3PXnw9ifjFVWk44a\nNQrHjh3D5MmTAaBZv0XX19zfcBp7OHFWVhYcHByajPnNb36D+Ph4hIaGorq6GgkJCSbHy8lrwoQJ\niI+PR0hICPR6PdLS0szOjIcPH46TJ09iwoQJkCQJSUlJzTrflStXml2gwsPDsXDhQkyZMgXV1dWI\niYlBmzZtTMZ4eXnh97//PT766CO4ubk1ufKwMbGxsVi0aBGqqqrQtWtXBAQENDlWzm+1tbW1SEtL\ng6enJ2bPng2dTodBgwaZvWX81ltvIS4uDg4ODnBycsKyZcsUzeux5ORkpKSkwN7eHu3bt8fSpUtN\njo+NjUViYiK2bNkCV1dXk6sC6+cl+jDulJQUzJ07FwaDAQ4ODnWfoZvi5eWF8PBwODk5YfDgwT/7\n4dvYezEhIQHLli1r9PrLee82jKmtrcXly5dNXv/GzjNv3jyT1180t8bGx8fHIy0trclr31jMihUr\nkJCQ0OT158PYnwzbsRERkc3jpnsiIrJ5LIZERGTzWAyJiMjmsRgSEZHNYzEkIiKbx2JIREQ2j8WQ\niIhsHoshERHZvP8PCofO8KLqmGYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1190a6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAFRCAYAAAALn8i+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwBwyv8D2VwzBOvP7JMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1190a6210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.heatmap(test[0,:,:].numpy())\n",
    "plt.figure()\n",
    "size_patch=5\n",
    "sb.heatmap(rel_distance_patch(size_patch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inpt=test[0,:,:]\n",
    "sx,sy=inpt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_patch=5\n",
    "data=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_patch=5\n",
    "patches=extract_patches(data,size_patch, zero_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 60000, 5, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_patch=[]        \n",
    "distances=[]\n",
    "X=data\n",
    "n_patches_per_graph=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=test[:1000,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Nystrom():\n",
    "    \n",
    "    def __init__(self,  n_components=50, iter_max=1000,random_state=None,size_patch=5,lr=0.001):\n",
    "        #self.eta = Variable(torch.Tensor(1.0/n_components *(np.ones((n_components,)))),requires_grad=True)\n",
    "        \n",
    "        self.eta = Variable(torch.Tensor(np.ones((n_components,))),requires_grad=True)\n",
    "        self.W = torch.Tensor(Variable(None))\n",
    "        self.sigma=None\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.iter_max=iter_max\n",
    "        self.print_lag=15\n",
    "        self.lr=lr\n",
    "        self.training_data=None\n",
    "        self.size_patch=size_patch\n",
    "        self.patches=None\n",
    "        self.distances=None\n",
    "    \t\t\n",
    "    def select_training_patches(self,X,n_patches_per_graph,patches=None):\n",
    "        #### X should be  a tensor (3D)\n",
    "        #### select training patches within the same graph\n",
    "       # X_train=torch.Tensor((X.size()[0],n_patches_per_graph,size_patch**2,size_patch**2))\n",
    "        id_patch=[]        \n",
    "        distances=[]\n",
    "        n_patches_per_graph=10\n",
    "        for i in range(X.size()[0]):\n",
    "            ## select at random 2 nodes in the adjacency matrix:\n",
    "            a=X.size()[1]\n",
    "            for j in range(n_patches_per_graph):\n",
    "                nx,ny=np.random.choice(range(X.size()[1]*X.size()[2]),2)\n",
    "                distances+=[(nx%a-ny%a)**2+(nx//a-ny//a)**2]\n",
    "                id_patch+=[[i,nx,ny]]\n",
    "        if patches==None:\n",
    "            patches=extract_patches(data,size_patch, zero_padding=True)\n",
    "        selected_patches=torch.Tensor(len(id_patch),2,size_patch**2)\n",
    "        for j in range(len(id_patch)):\n",
    "\n",
    "            while torch.sum(patches[id_patch[j][1],id_patch[j][0],:])==0 and torch.sum(patches[id_patch[j][2],id_patch[j][0],:])==0:\n",
    "                nx,ny=np.random.choice(range(X.size()[1]*X.size()[2]),2)\n",
    "                id_patch[j]=[id_patch[j][0],nx,ny]\n",
    "            selected_patches[j,0,:]=patches[id_patch[j][1],id_patch[j][0],:]\n",
    "            selected_patches[j,1,:]=patches[id_patch[j][2],id_patch[j][0],:]\n",
    "            \n",
    "        return extract_selected_patches(X,id_patch, self.size_patch),distances,patches\n",
    "            \n",
    "        \n",
    "    def init_W(self,X):\n",
    "        self.training_data,self.distances,self.patches=self.select_training_patches(X,50)\n",
    "        distances2=torch.sum((self.training_data[:,0,:]-self.training_data[:,1,:])**2,dim=1)\n",
    "        if self.sigma==None:\n",
    "        \t### set to be quantile\n",
    "            ### compute the distance between patches\n",
    "            self.sigma=np.percentile(distances2.numpy(),10)\n",
    "        X_tilde=torch.cat((self.training_data[:,0,:],self.training_data[:,1,:]), dim=0)\n",
    "        print(self.sigma)\n",
    "        km=KMeans(n_clusters=self.n_components)\n",
    "        inds=range(int(X_tilde.size()[0]))\n",
    "        np.random.shuffle(inds)\n",
    "        X_tilde=X_tilde[list(inds[:np.min([4000,len(inds)])]),:]\n",
    "        km.fit(X_tilde.numpy())\n",
    "        self.W=Variable(torch.Tensor(km.cluster_centers_), requires_grad=True)\n",
    "    \t\t\t\n",
    "    def fit(self, X, y=None, init=True):\n",
    "        rnd = check_random_state(self.random_state)\n",
    "        D=self.n_components\n",
    "        if init==True: self.init_W(X)\n",
    "        X_input=Variable(self.training_data, requires_grad=False)\n",
    "        N=X_input.size()[0]\n",
    "        n=X_input.size()[1]\n",
    "        expected_output=Variable(torch.Tensor(torch.exp(-torch.sum((X_input[:,0,:]-X_input[:,1,:])**2/self.sigma,1))),requires_grad=False)\n",
    "        loss_func = nn.MSELoss()\n",
    "        optimizer = optim.SGD([self.W,self.eta], lr=self.lr) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "        for t in range(self.iter_max):\n",
    "            output=(X_input[:,0,:].repeat(1, D)-self.W.view(1,D*n).repeat(N, 1))**2+(X_input[:,1,:].repeat(1, D)-self.W.view(1,D*n).repeat(N, 1))**2\n",
    "            output2=output.view(N,D,n)\n",
    "            output2=torch.exp(-1.0/sigma*torch.sum(output2,2))\n",
    "            output2=torch.matmul(output2,self.eta)\n",
    "            loss=loss_func(output2,expected_output)\n",
    "            if t%self.print_lag==0: print(t, loss.data[0])\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.eta=Variable((self.eta).clamp(0,1000),requires_grad=True)\n",
    "        print('final loss is:',loss.data[0])\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def fit_LBFGS(self,X,init=True):\n",
    "        rnd = check_random_state(self.random_state)\n",
    "        D=self.n_components\n",
    "        if init==True: self.init_W(X)\n",
    "        print(\"Initialization: done\")\n",
    "        D=self.n_components\n",
    "        X_input=self.training_data\n",
    "        N=X_input.size()[0]\n",
    "        n=X_input.size()[1]\n",
    "        expected_output=Variable(torch.exp(-torch.sum((X_input[:,0,:]-X_input[:,1,:])**2/(2*self.sigma),1)),requires_grad=False)\n",
    "        X_input=Variable(X_input, requires_grad=False)\n",
    "        loss_func = nn.MSELoss()\n",
    "        optimizer = optim.LBFGS([self.W,self.eta]) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "        batch_size=100\n",
    "        sigma=self.sigma\n",
    "        output_tot=torch.Tensor()\n",
    "        for t in range(self.iter_max):\n",
    "            overall_loss=0\n",
    "            for b in range(batch_size):\n",
    "                def closure():\n",
    "                    XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    output=(XX-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "                    \n",
    "                    output=torch.sum(output.view(-1,p_size**2),1)\n",
    "                    output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output)\n",
    "                    #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "                    weights=(2.0/(math.pi*sigma**2))**(D/2)*F.softmax(-self.eta)\n",
    "                    output2=torch.matmul(output2.view(-1,D),weights)\n",
    "                    loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])\n",
    "                    #overall_loss+=loss[0]\n",
    "                    #if t%self.print_lag==0: print(t, loss.data[0])\n",
    "                    optimizer.zero_grad() \n",
    "                    loss.backward()\n",
    "                    #optimizer.step()\n",
    "                    \n",
    "                optimizer.step(closure)\n",
    "                self.eta=(self.eta).clamp(0,1000)\n",
    "            print(t,overall_loss)\n",
    "        \n",
    "    def _get_kernel_params(self):\n",
    "        params = self.kernel_params\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def fit_i(self):\n",
    "        D=self.n_components\n",
    "        X_input=self.training_data\n",
    "        N=X_input.size()[0]\n",
    "        n=X_input.size()[1]\n",
    "        m=size_patch**2\n",
    "        expected_output=Variable(torch.exp(torch.Tensor([-1.0/(2*self.sigma)])*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1)),requires_grad=False)\n",
    "        expected_ouput=1.0/torch.sqrt(torch.mean(expected_output**2))*expected_output\n",
    "        X_input=Variable(X_input/255, requires_grad=False)\n",
    "        loss_func = nn.MSELoss()\n",
    "        optimizer = optim.Adamax([self.W,self.eta]) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "        batch_nb=50\n",
    "        batch_size=N//batch_nb\n",
    "        p_size=size_patch\n",
    "        output_tot=torch.Tensor()\n",
    "\n",
    "        for t in range(self.iter_max):\n",
    "                overall_loss=0\n",
    "            #def closure():\n",
    "                for b in range(batch_nb):\n",
    "                    #Cell.eta=F.relu(Cell.eta)\n",
    "                    XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "                    output=(XX-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-self.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "                    #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "                    #output2=output.view(N,D,n)\n",
    "                    output=torch.sum(output.view(-1,p_size**2),1)\n",
    "                    #print(output[:10])\n",
    "                    output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/self.sigma]))).expand_as(output)*output)\n",
    "\n",
    "                    #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "                    #weights=C.expand_as(eta)*F.softmax(-eta)\n",
    "                    output2=torch.matmul(output2.view(-1,D),F.relu(self.eta))\n",
    "                    #print(output2[:10])\n",
    "                    loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])+torch.sum((self.eta-F.relu(self.eta))**2)\n",
    "                    #+0.1/D*torch.sum(Cell.eta)\n",
    "                    #+2.0/D*torch.dot(Variable(torch.Tensor([1]*D)),Cell.eta)\n",
    "                    overall_loss+=loss[0]\n",
    "                    #STOP\n",
    "                    #if t%Cell.print_lag==0: print(t, loss.data[0])\n",
    "                    optimizer.zero_grad() \n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "                    #Cell.eta=F.relu(Cell.eta)\n",
    "\n",
    "\n",
    "                #optimizer.step()\n",
    "                #Cell.eta=(Cell.eta).clamp(0,1000)\n",
    "                print(t,overall_loss/(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data2=data-torch.mean(data.view(-1,data.size()[1]*data.size()[2]),0).view(28,28)\n",
    "S=torch.std(data2.view(-1,data.size()[1]*data.size()[2]),0)\n",
    "S[S==0]=1\n",
    "data2=1.0/S*data.view(-1,data.size()[1]*data.size()[2])\n",
    "data2=data2.view(-1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x128787b90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAFRCAYAAADgqHO9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr0mQbhTNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1287999d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.heatmap(data2[90,:,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.19582822323\n"
     ]
    }
   ],
   "source": [
    "Cell=Nystrom()\n",
    "#data=torch.Tensor([1.0/255]).expand_as(X)*X.type(torch.FloatTensor)\n",
    "#data=data-torch.mean(torch.mean(data.view(-1,data.size()[1]*data.size()[2]),0).view(28,28)).expand_as(data)\n",
    "Cell.init_W(data2)\n",
    "#Cell.fit_LBFGS(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "D=Cell.n_components\n",
    "X_input=Cell.training_data\n",
    "N=X_input.size()[0]\n",
    "n=X_input.size()[1]\n",
    "m=size_patch**2\n",
    "expected_output=Variable(torch.exp(torch.Tensor([-1.0/(2*Cell.sigma)])*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1)),requires_grad=False)\n",
    "expected_output=1.0/torch.sqrt(torch.mean(expected_output**2))*expected_output\n",
    "X_input=Variable(X_input, requires_grad=False)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adamax([Cell.W,Cell.eta]) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "batch_nb=50\n",
    "batch_size=N//batch_nb\n",
    "p_size=size_patch\n",
    "output_tot=torch.Tensor()\n",
    "\n",
    "for t in range(Cell.iter_max):\n",
    "        overall_loss=0\n",
    "    #def closure():\n",
    "        for b in range(batch_nb):\n",
    "            #Cell.eta=F.relu(Cell.eta)\n",
    "            XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "            YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "            output=(XX-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "            #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "            #output2=output.view(N,D,n)\n",
    "            output=torch.sum(output.view(-1,p_size**2),1)\n",
    "            #print(output[:10])\n",
    "            output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/Cell.sigma]))).expand_as(output)*output)\n",
    "            \n",
    "            #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "            #weights=C.expand_as(eta)*F.softmax(-eta)\n",
    "            output2=torch.matmul(output2.view(-1,D),F.relu(Cell.eta))\n",
    "            #print(output2[:10])\n",
    "            loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])+torch.sum(((Cell.eta-F.relu(Cell.eta))/torch.sum(F.relu(Cell.eta)))**2)\n",
    "            #+0.1/D*torch.sum(Cell.eta)\n",
    "            #+2.0/D*torch.dot(Variable(torch.Tensor([1]*D)),Cell.eta)\n",
    "            overall_loss+=loss[0]\n",
    "            #STOP\n",
    "            #if t%Cell.print_lag==0: print(t, loss.data[0])\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            #Cell.eta=F.relu(Cell.eta)\n",
    "             \n",
    "            if t%100==0 and b==49:\n",
    "                plt.figure()\n",
    "                plt.hist(expected_output[b*batch_size:(b+1)*batch_size].data.numpy(), color='red',bins=10)\n",
    "                plt.hist(output2.data.numpy(),bins=10)\n",
    "            \n",
    "        #optimizer.step()\n",
    "        #Cell.eta=(Cell.eta).clamp(0,1000)\n",
    "        print(t,overall_loss/(batch_nb))\n",
    "        #print(eta[:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(expected_output[b*batch_size:(b+1)*batch_size].data.numpy(), color='red',bins=10)\n",
    "plt.hist(output2.data.numpy(),bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cell.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    " \n",
    "##################################################################\n",
    "# Recursive generation of the Legendre polynomial of order n\n",
    "def Legendre(n,x):\n",
    "\tx=array(x)\n",
    "\tif (n==0):\n",
    "\t\treturn x*0+1.0\n",
    "\telif (n==1):\n",
    "\t\treturn x\n",
    "\telse:\n",
    "\t\treturn ((2.0*n-1.0)*x*Legendre(n-1,x)-(n-1)*Legendre(n-2,x))/n\n",
    " \n",
    "##################################################################\n",
    "# Derivative of the Legendre polynomials\n",
    "def DLegendre(n,x):\n",
    "\tx=array(x)\n",
    "\tif (n==0):\n",
    "\t\treturn x*0\n",
    "\telif (n==1):\n",
    "\t\treturn x*0+1.0\n",
    "\telse:\n",
    "\t\treturn (n/(x**2-1.0))*(x*Legendre(n,x)-Legendre(n-1,x))\n",
    "##################################################################\n",
    "# Roots of the polynomial obtained using Newton-Raphson method\n",
    "def LegendreRoots(polyorder,tolerance=1e-20):\n",
    "\tif polyorder<2:\n",
    "\t\terr=1 # bad polyorder no roots can be found\n",
    "\telse:\n",
    "\t\troots=[]\n",
    "\t\t# The polynomials are alternately even and odd functions. So we evaluate only half the number of roots. \n",
    "\t\tfor i in range(1,int(polyorder)/2 +1):\n",
    "\t\t\tx=cos(pi*(i-0.25)/(polyorder+0.5))\n",
    "\t\t\terror=10*tolerance\n",
    "\t\t        iters=0\n",
    "\t\t        while (error>tolerance) and (iters<1000):\n",
    "\t\t                dx=-Legendre(polyorder,x)/DLegendre(polyorder,x)\n",
    "\t\t                x=x+dx\n",
    "\t\t                iters=iters+1\n",
    "\t\t                error=abs(dx)\n",
    "\t\t\troots.append(x)\n",
    "\t\t# Use symmetry to get the other roots\n",
    "\t\troots=array(roots)\n",
    "\t\tif polyorder%2==0:\n",
    "\t\t\troots=concatenate( (-1.0*roots, roots[::-1]) )\n",
    "\t\telse:\n",
    "\t\t\troots=concatenate( (-1.0*roots, [0.0], roots[::-1]) )\n",
    "\t\terr=0 # successfully determined roots\n",
    "\treturn [roots, err]\n",
    "##################################################################\n",
    "# Weight coefficients\n",
    "def GaussLegendreWeights(polyorder):\n",
    "\tW=[]\n",
    "\t[xis,err]=LegendreRoots(polyorder)\n",
    "\tif err==0:\n",
    "\t\tW=2.0/( (1.0-xis**2)*(DLegendre(polyorder,xis)**2) )\n",
    "\t\terr=0\n",
    "\telse:\n",
    "\t\terr=1 # could not determine roots - so no weights\n",
    "\treturn [W, xis, err]\n",
    "##################################################################\n",
    "# The integral value \n",
    "# func \t\t: the integrand\n",
    "# a, b \t\t: lower and upper limits of the integral\n",
    "# polyorder \t: order of the Legendre polynomial to be used\n",
    "#\n",
    "def GaussLegendreQuadrature(func, polyorder, a, b):\n",
    "\t[Ws,xs, err]= GaussLegendreWeights(polyorder)\n",
    "\tif err==0:\n",
    "\t\tans=(b-a)*0.5*sum( Ws*func( (b-a)*0.5*xs+ (b+a)*0.5 ) )\n",
    "\telse: \n",
    "\t\t# (in case of error)\n",
    "\t\terr=1\n",
    "\t\tans=None\n",
    "\treturn [ans,err]\n",
    "##################################################################\n",
    "# The integrand - change as required\n",
    "def func(x):\n",
    "\treturn exp(-x**2/(2*sigma))\n",
    "##################################################################\n",
    "# \n",
    " \n",
    "order=100\n",
    "[Ws,xs,err]=GaussLegendreWeights(order)\n",
    "\n",
    " \n",
    "# Integrating the function\n",
    "[ans,err]=GaussLegendreQuadrature(func , order, -3,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cell.W.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(params, *args):\n",
    "    D=args[4]\n",
    "    #print(params)\n",
    "    W_opt = params[:-D]\n",
    "    eta_opt=params[-D:]\n",
    "    \n",
    "    W_opt=W_opt.reshape([D,-1])\n",
    "    eta_opt=eta_opt.reshape([D,1])\n",
    "    print(W_opt.shape)\n",
    "    XX = args[0]\n",
    "    YY = args[1]  #.reshape([100,25,1]),D)\n",
    "    sigma=args[2]\n",
    "    N=XX.shape[0]\n",
    "    expected_ouput = args[3]\n",
    "    test=np.array([((XX[i,:]-W_opt)**2)+((YY[i,:]-W_opt)**2)  for i in range(N)])\n",
    "    #print((np.exp(-1.0/sigma*np.sum(test,2))).shape)\n",
    "    #print(eta.shape)\n",
    "    #print (np.sum(test,2).shape)\n",
    "    res=(expected_ouput_t -((np.exp(-1.0/sigma*np.sum(test,2)))).dot(eta))\n",
    "    return np.sum(res**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D=Cell.n_components\n",
    "X_input=Cell.training_data\n",
    "N=X_input.size()[0]\n",
    "n=X_input.size()[1]\n",
    "m=size_patch**2\n",
    "expected_output=Variable(torch.exp(torch.Tensor([-1.0/(2*Cell.sigma)])*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1)),requires_grad=False)\n",
    "X_input=Variable(X_input, requires_grad=False)\n",
    "loss_func = nn.MSELoss()\n",
    "eta=Variable(torch.Tensor([1]*D),requires_grad=True)\n",
    "W=Variable(W_storage.data,requires_grad=True)\n",
    "optimizer = optim.SGD([Cell.W,Cell.eta],lr=10) # instantiate optimizer with model params + learning rate (SGD for the moment)\n",
    "batch_size=100\n",
    "p_size=size_patch\n",
    "output_tot=torch.Tensor()\n",
    "print(W.size())\n",
    "W=Cell.W.data.numpy().reshape([-1,1])\n",
    "eta=Cell.eta.data.numpy().reshape([-1,1])\n",
    "for t in range(Cell.iter_max):\n",
    "        overall_loss=0\n",
    "        print(overall_loss)\n",
    "    #def closure():\n",
    "        for b in range(batch_size):\n",
    "            #Cell.eta=F.relu(Cell.eta)\n",
    "            XX=X_input[b*batch_size:(b+1)*batch_size,0,:] #.contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "            YY=X_input[b*batch_size:(b+1)*batch_size,1,:] #.contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "            #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "            params=np.array(list(W)+list(eta))\n",
    "            #print(len(params))\n",
    "            expected_ouput_t=expected_output[b*batch_size:(b+1)*batch_size].data.numpy()\n",
    "            args=(X_input[b*batch_size:(b+1)*batch_size,0,:].data.numpy(),\\\n",
    "                X_input[b*batch_size:(b+1)*batch_size,1,:].data.numpy(),\\\n",
    "                Cell.sigma,\\\n",
    "                expected_ouput_t,\\\n",
    "                50)\n",
    "            initial_values = np.array(list(W)+list(eta))\n",
    "            mybounds = [(None,None)]*len(W)+[(0,None)]*len(eta)\n",
    "            x,f,d = scipy.optimize.fmin_l_bfgs_b(func, x0=initial_values, args=args, bounds=mybounds,approx_grad=True)\n",
    "            W=x[:-50]\n",
    "            eta=x[-50:]\n",
    "\n",
    "\n",
    "            #loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])+torch.sum((Cell.eta-F.relu(Cell.eta))**2)\n",
    "            #+0.1/D*torch.sum(Cell.eta)\n",
    "            #+2.0/D*torch.dot(Variable(torch.Tensor([1]*D)),Cell.eta)\n",
    "            overall_loss+=f\n",
    "            #STOP\n",
    "            #if t%Cell.print_lag==0: print(t, loss.data[0])\n",
    "            #optimizer.zero_grad() \n",
    "            #loss.backward(retain_graph=True)\n",
    "            #optimizer.step()\n",
    "            #Cell.eta=F.relu(Cell.eta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XX=X_input[:,0,:] #.contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "#YY=X_input[b*batch_size:(b+1)*batch_size,1,:] #.contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "#output=(XX-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "W=Cell.W.data.numpy().reshape([-1,1])\n",
    "eta=Cell.eta.data.numpy().reshape([-1,1])\n",
    "params=np.array(list(W)+list(eta))\n",
    "#print(len(params))\n",
    "expected_ouput_t=expected_output.data.numpy()\n",
    "args=(X_input[:,0,:].data.numpy(),\\\n",
    "    X_input[:,1,:].data.numpy(),\\\n",
    "    Cell.sigma,\\\n",
    "    expected_ouput_t,\\\n",
    "    50)\n",
    "initial_values = np.array(list(W)+list(eta))\n",
    "mybounds = [(None,None)]*len(W)+[(0,None)]*len(eta)\n",
    "print('optimizing')\n",
    "x,f,d = scipy.optimize.fmin_l_bfgs_b(func, x0=initial_values, args=args, bounds=mybounds,approx_grad=True,maxiter=5000)\n",
    "W=x[:-50]\n",
    "eta=x[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testy2=np.array([((testy[i,:]-W.data.numpy())**2)+((testy[i,:]-W.data.numpy())**2) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W=Cell.W.data\n",
    "eta=Cell.eta.data\n",
    "params=[W,eta]\n",
    "args=(X_input[b*batch_size:(b+1)*batch_size,0,:].data.numpy(),\\\n",
    "      X_input[b*batch_size:(b+1)*batch_size,1,:].data.numpy(),\\\n",
    "      Cell.sigma,\\\n",
    "      expected_ouput_t,\\\n",
    "      50)\n",
    "initial_values = np.array(list(Cell.W.data.numpy().reshape([-1,]))+list(Cell.eta.data))\n",
    "mybounds = [(None,None)]*2500+[(0,None)]*100\n",
    "x,f,d = scipy.optimize.fmin_l_bfgs_b(func, x0=initial_values, args=args, bounds=mybounds,approx_grad=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mybounds = [(None,None)*np.ones(Cell.W.data.shape), [(0,None)]*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.exp(torch.Tensor([-1.0/(2*sigma)])*torch.sum((X_input[:,0,:]-X_input[:,1,:])**2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=XX.shape[0]\n",
    "test=np.array([((W.numpy()-XX[i,:])**2)+((YY[i,:]-W.numpy())**2)  for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res=(expected_ouput_t -((eta.numpy().T).dot(np.exp(-1.0/sigma*np.sum(test,2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wfinal=x[:-D].reshape([D,25])\n",
    "etafinal=x[-D:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=np.array([((Wfinal-XX[i,:])**2)+((YY[i,:]-Wfinal)**2)  for i in range(N)])\n",
    "testy2=((etafinal.T).dot(np.exp(-1.0/sigma*np.sum(test,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### convergence feels super long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma=Cell.sigma\n",
    "XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "output=(XX-W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "            #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "            #output2=output.view(N,D,n)\n",
    "output=torch.sum(output.view(-1,p_size**2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XX,YY,sigma=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([Cell.W,Cell.eta], lr=0.001) # instantiate optimizer with model params + learning rate (SGD for the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "#for b in range(batch_size):\n",
    "if True:\n",
    "    XX=X_input[b*batch_size:(b+1)*batch_size,0,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "    YY=X_input[b*batch_size:(b+1)*batch_size,1,:].contiguous().view((1,batch_size,p_size**2)).expand(D,batch_size,p_size**2)\n",
    "    output=(XX-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2+(YY-Cell.W.view(D,1,p_size**2).expand(D,batch_size,p_size**2))**2\n",
    "    #output=(XX-Cell.W.view(D,1,p_size**2).expand(D,X_input.size()[0],p_size**2))**2+(X_input[:,1,:].repeat(1, D)-Cell.W.view(1,D*n).repeat(N, 1))**2\n",
    "    #output2=output.view(N,D,n)\n",
    "    output=torch.sum(output.view(-1,p_size**2),1)\n",
    "    output2=torch.exp(Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output)\n",
    "    #output2=torch.exp((-1.0/sigma).expand_as(output)*torch.sum(output.view(-1,p_size**2),1))\n",
    "    output2=torch.matmul(output2.view(-1,D),Cell.eta)\n",
    "    loss=loss_func(output2,expected_output[b*batch_size:(b+1)*batch_size])\n",
    "    if t%Cell.print_lag==0: print(t, loss.data[0])\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    #optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output=torch.sum(output.view(-1,p_size**2),1)\n",
    "output2=Variable(torch.Tensor(np.array([-1.0/sigma]))).expand_as(output)*output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cell.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cell.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_image=data.size()[1:]\n",
    "selected_patches=torch.Tensor(len(id_patch),2,size_patch**2)\n",
    "for j in range(len(id_patch)):\n",
    "        selected_patches[j,0,:]=patches[id_patch[j][1],id_patch[j][0],:]\n",
    "        selected_patches[j,1,:]=patches[id_patch[j][2],id_patch[j][0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_selected_patches(data,id_patch,size_patch, zero_padding=True):\n",
    "    \n",
    "    size_image=data.size()[1:]\n",
    "    center_x=id_patch[1]//size_image[0]\n",
    "    center_y=id_patch[1]%size_image[0]\n",
    "    selected_patches=torch.Tensor((len(id_patches),2,size_patch**2))\n",
    "    patches=extract_patches(data,size_patch, zero_padding=True)\n",
    "    for j in range(len(id_patch)):\n",
    "        selected_patches[j,0,:]=patches[center_x[j],id_patch[j][0],:]\n",
    "        selected_patches[j,1,:]=patches[center_y[j],id_patch[j][0],:]\n",
    "    return patches[id_patch[0],center_x,center_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "size_image=data.size()[1:]\n",
    "a=size_patch+size_image[0]\n",
    "b=size_patch+size_image[1]\n",
    "padded_image=torch.Tensor(np.zeros((data.size()[0],a,b)))\n",
    "padded_image[:,size_patch//2:(size_patch//2)+size_image[0],size_patch//2:(size_patch//2)+size_image[1]]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx,ny=padded_image.size()[1:]\n",
    "patches=torch.Tensor(np.array([padded_image[:,ii:ii+size_patch,jj:jj+size_patch].numpy() for ii in np.arange(0,nx-size_patch,1) for jj in np.arange(0,ny-size_patch,1) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches=patches.view([patches.size()[0],patches.size()[1],size_patch**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=test\n",
    "size_patch=5\n",
    "import numpy as np\n",
    "patches=extract_patches(data,size_patch, zero_padding=True)\n",
    "patches=patches.view([patches.size()[0],patches.size()[1],size_patch**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_shape=size_patch\n",
    "#spacing=np.min([p_shape, data.size()[2]//p_shape]) \n",
    "spacing=p_shape//2\n",
    "gamma=2\n",
    "D=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches[100,30,:].norm(p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W=torch.Tensor(np.random.sample(D*size_patch**2).reshape([D,size_patch**2]))\n",
    "\n",
    "eta=torch.Tensor(np.random.dirichlet([1.0/D]*D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_p,n_d=patches.size()[:2]\n",
    "norm = torch.Tensor(np.array([patches[i,j,:].norm(p=2) for i in range(n_p) for j in range(n_d)]))\n",
    "\n",
    "norm2 = patches.view((n_p*n_d,size_patch**2)).norm(p=2,dim=1).view((n_p,n_d,1))\n",
    "\n",
    "norm3=torch.max(norm2, torch.Tensor(np.ones(norm2.size())))\n",
    "\n",
    "norm_patches=patches.div(norm3.expand_as(patches))\n",
    "\n",
    "W2=W.expand(n_d*n_p,D,25)\n",
    "norm_patches2=norm_patches.view((n_d*n_p,1,size_patch**2)).expand(n_d*n_p,D,size_patch**2)\n",
    "batch_size=10000\n",
    "testy=torch.Tensor((n_d*n_p*D))\n",
    "output2=torch.Tensor((n_d*n_p*D))\n",
    "sigma=1\n",
    "\n",
    "\n",
    "\n",
    "for b in range(n_d*n_p//batch_size):\n",
    "    print(b)\n",
    "    testy[b*batch_size*D:(b+1)*batch_size*D]=torch.sum(((norm_patches2[b*batch_size:(b+1)*batch_size,:,:]\\\n",
    "            -W.expand_as(norm_patches2[b*batch_size:(b+1)*batch_size,:,:]))**2).contiguous().view((-1,25)),dim=1)\n",
    "    \n",
    "    output2[b*batch_size*D:(b+1)*batch_size*D]=torch.exp(-1.0/sigma*testy[b*batch_size*D:(b+1)*batch_size*D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2=testy.view(n_p,n_d,D)\n",
    "sigma=1\n",
    "output2=torch.exp(-1.0/sigma*output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output2=torch.matmul(output2,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zeta=norm.view((n_d,n_p))*(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output2=torch.matmul(output2,eta)\n",
    "#zeta=norm.mmul(output2)\n",
    "#patches=[c.reshape([1,-1]) for c in extract_patches_2d(zeta.numpy(),[p_shape,p_shape])]\n",
    "beta=gamma*spacing  ### what is that spacing? Where is it defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_map=2.0/np.sqrt(math.pi)*torch.sum(torch.exp(-1.0/beta*torch.Tensor(model_patch))*zeta,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output2=output.view(N,p_shape**2,D)\n",
    "output2=torch.exp(-1.0/self.Kernels[id_map].sigma*torch.sum(output2,1))\n",
    "output2=torch.matmul(output2,self.Kernels[id_map].eta)\n",
    "zeta=norm.mmul(output2)\n",
    "patches=[c.reshape([1,-1]) for c in extract_patches_2d(zeta.numpy(),[p_shape,p_shape])]\n",
    "beta=gamma*spacing  ### what is that spacing? Where is it defined?\n",
    "output_map=self.model_patch[k]*patches\n",
    "return output_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ### start by extracting patches from image\n",
    "    patches=[c.reshape([1,-1]) for c in extract_patches_2d(xi.numpy(),[p_shape,p_shape])]\n",
    "    patches=torch.Tensor(patches)\n",
    "    patches=patches.squeeze()\n",
    "    ### subsamples patches?\n",
    "    \n",
    "    ### l2 normalized version of the patches\n",
    "    norm = patches.norm(p=2, dim=0, keepdim=True)\n",
    "    norm_patches=patches.div(norm.expand_as(patches))\n",
    "    N=norm_patches.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_activation_map(im,xi,id_map):\n",
    "    ### have to generalize this guy to take as input lots of images in the right tensor shape\n",
    "    p_shape=self.patch_shape[id_map]\n",
    "    spacing=np.min([im.size()[0]//p_shape, im.size()[1]//p_shape]) \n",
    "    gamma=2\n",
    "    D=self.Kernels[id_map].n_components\n",
    "    ### start by extracting patches from image\n",
    "    patches=[c.reshape([1,-1]) for c in extract_patches_2d(xi.numpy(),[p_shape,p_shape])]\n",
    "    patches=torch.Tensor(patches)\n",
    "    patches=patches.squeeze()\n",
    "    ### subsamples patches?\n",
    "    \n",
    "    ### l2 normalized version of the patches\n",
    "    norm = patches.norm(p=2, dim=0, keepdim=True)\n",
    "    norm_patches=patches.div(norm.expand_as(patches))\n",
    "    N=norm_patches.size()[0]\n",
    "    output=(norm_patches.repeat(1, D).view(N,p_shape**2,D)-self.Kernels[id_map].W.view(p_shape**2,D).repeat(0, N))**2\n",
    "    output2=output.view(N,p_shape**2,D)\n",
    "    output2=torch.exp(-1.0/self.Kernels[id_map].sigma*torch.sum(output2,1))\n",
    "    output2=torch.matmul(output2,self.Kernels[id_map].eta)\n",
    "    zeta=norm.mmul(output2)\n",
    "    patches=[c.reshape([1,-1]) for c in extract_patches_2d(zeta.numpy(),[p_shape,p_shape])]\n",
    "    beta=gamma*spacing  ### what is that spacing? Where is it defined?\n",
    "    output_map=self.model_patch[k]*patches\n",
    "    return output_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
